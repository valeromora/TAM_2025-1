{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8gfFa3mdhl6uZ5vaMEdmK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valeromora/TAM_2025-1/blob/main/Final%20Proyect/Final_proyect_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set up"
      ],
      "metadata": {
        "id": "Ofgbef-NapRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries if they are not already installed\n",
        "!pip install PyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbFQJK-8pSvU",
        "outputId": "5724168a-ae32-49a2-8be7-ee64b60d8197"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.11/dist-packages (1.3.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.11/dist-packages (from PyDrive) (2.174.0)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.11/dist-packages (from PyDrive) (6.0.2)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.2->PyDrive) (2.38.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.2->PyDrive) (2.25.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.2->PyDrive) (4.2.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.9.1)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from oauth2client>=4.0.0->PyDrive) (1.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (1.26.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client>=1.2->PyDrive) (5.5.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client>=1.2->PyDrive) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhRt8kx9wIvt",
        "outputId": "75772734-99f1-4bfb-cfb7-ad31f05c7eab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kOgxSDHxV78",
        "outputId": "65aa554c-9d8b-4cea-f001-dc781cf699c0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n",
            "Downloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m470.2/470.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 4.1.0\n",
            "    Uninstalling sentence-transformers-4.1.0:\n",
            "      Successfully uninstalled sentence-transformers-4.1.0\n",
            "Successfully installed faiss-cpu-1.11.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 sentence-transformers-5.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbT3_kKcamUD",
        "outputId": "29831739-1b44-4d56-8a74-ac66dd79c2e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Database"
      ],
      "metadata": {
        "id": "Nsck3Q8wIE44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://drive.google.com/file/d/1x6HASgMM9WqaglFR-zH2_LU6eRs4l_tq/view?usp=sharing\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download the file\n",
        "file_id = '1x6HASgMM9WqaglFR-zH2_LU6eRs4l_tq'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('downloaded_file.zip')\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile('downloaded_file.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('/content/')\n",
        "\n",
        "# List the extracted files to confirm\n",
        "!ls /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObIrVAqIbUp5",
        "outputId": "4ba9d014-dd7f-4b62-fb69-54f90bd4664c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'12EE54A_6.2.1 - Safely managed sanitation & hand-washing'\n",
            "'1548EA3_6.1.1 - Safely managed drinking water'\n",
            "'16BBF41_3.4.2 - Suicide'\n",
            "'1772666_3.1.2 - Births attended by skilled health personnel'\n",
            "'1F96863_3.4.1 - cardiovascular disease, cancer, diabetes or chronic respiratory disease'\n",
            "'217795A_3.C.1 - Health worker density and distribution'\n",
            "'2322814_3.2.1 - Under-five mortality rate'\n",
            "'2D6FBE4_3.3.5 - Neglected tropical diseases'\n",
            "'361734E_16.1.1 - Intentional homicide'\n",
            "'442CEA8_3.3.3 - Malaria'\n",
            "'45CA7C8_3.C.1 - Health worker density and distribution'\n",
            "'5C8435F_3.C.1 - Health worker density and distribution'\n",
            "'5F8A486_2.2.1 - Stunting'\n",
            " 608DE39\n",
            "'6A64C9A_7.1.2 - Clean fuels'\n",
            "'75DDA77_3.A.1 - Age-standardized prevalence of tobacco use'\n",
            "'77D059C_3.3.1 - HIV infections'\n",
            "'8074BD9_3.7.1 - Women satisfied with modern methods'\n",
            "'84FD3DE_3.9.3 - Unintentional poisoning'\n",
            "'A37BDD6_6.3.1 - Safely treated wastewater'\n",
            "'A4C49D3_3.2.2- Neonatal mortality rate'\n",
            "'AC597B1_3.1.1 - Maternal mortality ratio'\n",
            "'B9C6C79_1.a.2 - Government spending essential services'\n",
            "'BBF3A64_3.B.2 - Development assistance'\n",
            " BEFA58B\n",
            "'C288D13_3.3.2 - Tuberculosis'\n",
            "'D1223E8_6.2.1 - Safely managed sanitation & hand-washing'\n",
            "'D6176E2_3.6.1 - Deaths due to road traffic injuries'\n",
            " downloaded_file.zip\n",
            "'E0D4E17_5.2.2 - Sexual violence by persons other than an intimate partner (last 12 months)'\n",
            "'E2FC6D7_3.9.1 - Household and ambient air pollution'\n",
            "'ED50112_3.9.2 - Unsafe water, unsafe sanitation and lack of hygiene'\n",
            "'EE6F72A_3.5.2 - Alcohol'\n",
            " EF93DDB\n",
            "'F513188_3.3.4 - Hepatitis B'\n",
            "'F810947_11.6.2 - Fine particulate matter'\n",
            "'F8524F2_5.2.1 - Intimate partner violence (last 12 months)'\n",
            "'F8E084C_3.B.1 - Access to affordable medicines and vaccines'\n",
            "'FC5231F_2.2.2 - Malnutrition'\n",
            " sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparaci√≥n archivos\n",
        "### Extracci√≥n y enriquecimiento de informaci√≥n por carpeta\n",
        "Este bloque de c√≥digo recorre cada carpeta del dataset, busca archivos relevantes (`Metadata`, `Dictionary`, `Code list`, `Dataset`) y construye documentos enriquecidos en texto plano.\n",
        "\n",
        "- Se extraen definiciones de variables desde el archivo **Data Dictionary**.\n",
        "- Se interpretan valores codificados usando el **Code List** (si est√° presente).\n",
        "- Se incluye el contexto general desde el archivo **Metadata**.\n",
        "- Cada fila del Dataset es enriquecida con descripciones legibles para facilitar su comprensi√≥n y futura vectorizaci√≥n.\n",
        "- El resultado se almacena en una lista de diccionarios llamada `documents`, lista para dividirse en _chunks_.\n",
        "\n",
        "Esto permite aprovechar mejor los datos, incluso si vienen organizados de manera heterog√©nea.\n"
      ],
      "metadata": {
        "id": "CAV7g8tRusfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/content\"\n",
        "csv_count = 0\n",
        "folders_with_csv = {}\n",
        "\n",
        "for folder in os.listdir(base_path):\n",
        "    folder_path = os.path.join(base_path, folder)\n",
        "    if os.path.isdir(folder_path):\n",
        "        csv_files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
        "        csv_count += len(csv_files)\n",
        "        folders_with_csv[folder] = csv_files\n",
        "\n",
        "print(f\"Total de archivos .csv encontrados: {csv_count}\")\n",
        "print(f\"Resumen por carpeta:\")\n",
        "for folder, files in folders_with_csv.items():\n",
        "    print(f\"{folder}: {len(files)} archivos\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlX54D2z_s6H",
        "outputId": "d2eb88ee-f941-4649-9f56-520ed7a8e46d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de archivos .csv encontrados: 139\n",
            "Resumen por carpeta:\n",
            ".config: 0 archivos\n",
            "1772666_3.1.2 - Births attended by skilled health personnel: 3 archivos\n",
            "F810947_11.6.2 - Fine particulate matter: 4 archivos\n",
            "AC597B1_3.1.1 - Maternal mortality ratio: 3 archivos\n",
            "84FD3DE_3.9.3 - Unintentional poisoning: 4 archivos\n",
            "BEFA58B: 4 archivos\n",
            "F513188_3.3.4 - Hepatitis B: 3 archivos\n",
            "B9C6C79_1.a.2 - Government spending essential services: 3 archivos\n",
            "12EE54A_6.2.1 - Safely managed sanitation & hand-washing: 4 archivos\n",
            "2322814_3.2.1 - Under-five mortality rate: 4 archivos\n",
            "361734E_16.1.1 - Intentional homicide: 4 archivos\n",
            "D1223E8_6.2.1 - Safely managed sanitation & hand-washing: 4 archivos\n",
            "6A64C9A_7.1.2 - Clean fuels: 4 archivos\n",
            "EF93DDB: 4 archivos\n",
            "5F8A486_2.2.1 - Stunting: 3 archivos\n",
            "C288D13_3.3.2 - Tuberculosis: 3 archivos\n",
            "A37BDD6_6.3.1 - Safely treated wastewater: 3 archivos\n",
            "A4C49D3_3.2.2- Neonatal mortality rate: 4 archivos\n",
            "F8524F2_5.2.1 - Intimate partner violence (last 12 months): 3 archivos\n",
            "D6176E2_3.6.1 - Deaths due to road traffic injuries: 3 archivos\n",
            "EE6F72A_3.5.2 - Alcohol: 4 archivos\n",
            "BBF3A64_3.B.2 - Development assistance: 3 archivos\n",
            "ED50112_3.9.2 - Unsafe water, unsafe sanitation and lack of hygiene: 4 archivos\n",
            "75DDA77_3.A.1 - Age-standardized prevalence of tobacco use: 4 archivos\n",
            "77D059C_3.3.1 - HIV infections: 4 archivos\n",
            "16BBF41_3.4.2 - Suicide: 4 archivos\n",
            "2D6FBE4_3.3.5 - Neglected tropical diseases: 3 archivos\n",
            "1548EA3_6.1.1 - Safely managed drinking water: 4 archivos\n",
            "442CEA8_3.3.3 - Malaria: 3 archivos\n",
            "608DE39: 4 archivos\n",
            "FC5231F_2.2.2 - Malnutrition: 4 archivos\n",
            "5C8435F_3.C.1 - Health worker density and distribution: 3 archivos\n",
            "217795A_3.C.1 - Health worker density and distribution: 3 archivos\n",
            "45CA7C8_3.C.1 - Health worker density and distribution: 3 archivos\n",
            "1F96863_3.4.1 - cardiovascular disease, cancer, diabetes or chronic respiratory disease: 4 archivos\n",
            "E0D4E17_5.2.2 - Sexual violence by persons other than an intimate partner (last 12 months): 3 archivos\n",
            "F8E084C_3.B.1 - Access to affordable medicines and vaccines: 3 archivos\n",
            "E2FC6D7_3.9.1 - Household and ambient air pollution: 4 archivos\n",
            "8074BD9_3.7.1 - Women satisfied with modern methods: 4 archivos\n",
            "sample_data: 4 archivos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ruta base donde se encuentran las carpetas con los datasets y archivos relacionados\n",
        "base_path = \"/content\"\n",
        "\n",
        "# Lista que almacenar√° los documentos generados por cada carpeta,\n",
        "# donde cada documento es un texto enriquecido con metadata, definiciones y datos\n",
        "documents = []\n",
        "\n",
        "# Recorremos cada carpeta dentro de la ruta base\n",
        "for folder in os.listdir(base_path):\n",
        "    folder_path = os.path.join(base_path, folder)\n",
        "    # Saltamos si no es carpeta (p.ej., archivos sueltos)\n",
        "    if not os.path.isdir(folder_path):\n",
        "        continue\n",
        "\n",
        "    # Diccionarios de mapeo para traducir y dar contexto:\n",
        "    # data_dict_map: mapea nombre de variable ‚Üí definici√≥n textual\n",
        "    data_dict_map = {}\n",
        "    # code_list_map: mapea (dimensi√≥n, c√≥digo) ‚Üí descripci√≥n del c√≥digo\n",
        "    code_list_map = {}\n",
        "\n",
        "    # Variable para acumular el texto de metadata (solo un archivo por carpeta)\n",
        "    metadata_text = \"\"\n",
        "\n",
        "    # B√∫squeda y extracci√≥n del archivo de Metadata para la carpeta actual\n",
        "    # Solo se procesa el primer archivo que cumpla la condici√≥n\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if \"Metadata\" in filename and filename.endswith(\".csv\"):\n",
        "            try:\n",
        "                df_meta = pd.read_csv(os.path.join(folder_path, filename))\n",
        "                metadata_text += f\"üìÑ Metadata de {filename}:\\n\"\n",
        "                # Recorremos las filas sin √≠ndice para extraer clave y valor\n",
        "                for row in df_meta.itertuples(index=False):\n",
        "                    if len(row) >= 2:\n",
        "                        key = str(row[0]).strip()\n",
        "                        value = str(row[1]).strip()\n",
        "                        metadata_text += f\"{key}: {value}\\n\"\n",
        "            except Exception as e:\n",
        "                print(f\"No se pudo leer metadata en {folder}: {e}\")\n",
        "            break  # Solo procesamos un archivo Metadata por carpeta\n",
        "\n",
        "    # Primera pasada: cargamos diccionarios para darle sentido a los datos\n",
        "    for filename in os.listdir(folder_path):\n",
        "        # Procesamos solo archivos CSV\n",
        "        if not filename.endswith(\".csv\"):\n",
        "            continue\n",
        "\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "        except Exception as e:\n",
        "            print(f\"No se pudo leer {filename}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Si es un archivo Data Dictionary, cargamos definiciones de variables\n",
        "        if \"Dictionary\" in filename or \"Data Dictionary\" in filename:\n",
        "            for row in df.itertuples(index=False):\n",
        "                if len(row) >= 2:\n",
        "                    var = str(row[0]).strip()\n",
        "                    definition = str(row[1]).strip()\n",
        "                    # Guardamos mapeo: nombre_variable -> definici√≥n\n",
        "                    data_dict_map[var] = definition\n",
        "\n",
        "        # Si es un archivo Code List, cargamos descripciones de c√≥digos por dimensi√≥n\n",
        "        elif \"Code list\" in filename:\n",
        "            for row in df.itertuples(index=False):\n",
        "                if len(row) >= 4:\n",
        "                    dim = str(row[0]).strip()     # Dimensi√≥n o categor√≠a\n",
        "                    key = str(row[1]).strip()     # C√≥digo o clave\n",
        "                    name = str(row[2]).strip()    # Nombre o etiqueta\n",
        "                    desc = str(row[3]).strip()    # Descripci√≥n detallada\n",
        "                    # Guardamos mapeo: (dimensi√≥n, c√≥digo) -> descripci√≥n enriquecida\n",
        "                    code_list_map[(dim, key)] = f\"{name}: {desc}\"\n",
        "\n",
        "    # Segunda pasada: procesamos archivos que contengan los datos en s√≠\n",
        "    for filename in os.listdir(folder_path):\n",
        "        # Saltamos si no es CSV o si es Dictionary o Code list (ya procesados)\n",
        "        if not filename.endswith(\".csv\") or \"Dictionary\" in filename or \"Code list\" in filename:\n",
        "            continue\n",
        "\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "        except Exception as e:\n",
        "            print(f\"No se pudo leer {filename}: {e}\")\n",
        "            continue\n",
        "\n",
        "        text_content = \"\"\n",
        "\n",
        "        # Si por alg√∫n motivo volvemos a encontrar metadata, la agregamos\n",
        "        if \"Metadata\" in filename:\n",
        "            text_content += f\"üßæ Metadata extra√≠da de {filename}:\\n\"\n",
        "            for row in df.itertuples(index=False):\n",
        "                if len(row) >= 2:\n",
        "                    key = str(row[0]).strip()\n",
        "                    value = str(row[1]).strip()\n",
        "                    text_content += f\"{key}: {value}\\n\"\n",
        "\n",
        "        # Si es un dataset con datos, construimos textos enriquecidos\n",
        "        elif \"Dataset\" in filename:\n",
        "            text_content += f\"üìä Datos enriquecidos de {filename}:\\n\"\n",
        "            # Recorremos fila por fila\n",
        "            for row in df.itertuples(index=False):\n",
        "                row_description = []\n",
        "                # Para cada columna, buscamos su definici√≥n y posible descripci√≥n de valor\n",
        "                for col in df.columns:\n",
        "                    # Valor en la celda actual\n",
        "                    value = getattr(row, col, \"\")\n",
        "                    # Nombre legible para la variable, si existe\n",
        "                    label = data_dict_map.get(col, col)\n",
        "\n",
        "                    # Si el valor es una cadena y existe descripci√≥n en code_list para esa dimensi√≥n y valor\n",
        "                    if isinstance(value, str) and (col, value) in code_list_map:\n",
        "                        value_desc = code_list_map[(col, value)]\n",
        "                        row_description.append(f\"{label}: {value_desc}\")\n",
        "                    else:\n",
        "                        # Si no hay descripci√≥n especial, usamos el valor tal cual\n",
        "                        row_description.append(f\"{label}: {value}\")\n",
        "\n",
        "                # Concatenamos la descripci√≥n enriquecida para la fila completa\n",
        "                text_content += \" | \".join(row_description) + \"\\n\"\n",
        "\n",
        "        else:\n",
        "            # Archivos no reconocidos se reportan para control\n",
        "            print(f\"Archivo sin tipo conocido: {filename}\")\n",
        "            continue\n",
        "\n",
        "        # Finalmente, agregamos a la lista de documentos con su fuente y contenido enriquecido\n",
        "        documents.append({\n",
        "            \"source\": f\"{folder}/{filename}\",\n",
        "            \"content\": metadata_text + \"\\n\" + text_content\n",
        "        })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjxNZePbpPf-",
        "outputId": "912b6dac-899d-4240-e928-a53aad489227"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo sin tipo conocido: mnist_test.csv\n",
            "Archivo sin tipo conocido: california_housing_train.csv\n",
            "Archivo sin tipo conocido: california_housing_test.csv\n",
            "Archivo sin tipo conocido: mnist_train_small.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total documentos extra√≠dos: {len(documents)}\")\n",
        "print(documents[0]['source'])\n",
        "print(documents[0]['content'][:1000])  # muestra los primeros caracteres del primer documento"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrY33mO9uVuc",
        "outputId": "709a3496-506c-435e-d7a2-bf7d44fadca9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total documentos extra√≠dos: 76\n",
            "1772666_3.1.2 - Births attended by skilled health personnel/170_1772666_Metadata_2025-04-07.csv\n",
            "üìÑ Metadata de 170_1772666_Metadata_2025-04-07.csv:\n",
            "Name: Proportion of births attended by skilled health personnel (%)\n",
            "Short name: Proportion of births attended by skilled health personnel (%)\n",
            "Indicator unique identifier: 1772666\n",
            "Indicator codes: MDG_0000000025\n",
            "Also known as: SDG indicator 3.1.2\n",
            "SDG Goal: 3.1.2 ‚Äì Births attended by skilled health personnel\n",
            "Short description: Proportion of births attended by skilled health personnel.\r\n",
            "(SDG 3.1.2)\n",
            "Definition: Proportion of births attended by skilled health personnel (generally doctors, nurses or midwives but can refer to other health professionals providing childbirth care) is the proportion of childbirths attended by professional health personnel.\r\n",
            "According to the current definition (1) these are competent maternal and newborn health (MNH) professionals educated, trained and regulated to national and international standards. They are competent to: (i) provide and promote evidence-based, human-rights based, quality, socio-culturally sen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunking\n",
        "\n",
        "Se busca dividir cada content del documento en trozos m√°s peque√±os.\n",
        "* Divide el texto en grupos de 500 palabras\n",
        "\n",
        "* Repite las √∫ltimas 50 palabras del chunk anterior (para mantener contexto)"
      ],
      "metadata": {
        "id": "ZDYSuRRVvk7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_CHUNK_WORDS = 500\n",
        "OVERLAP = 50\n",
        "\n",
        "chunks = []\n",
        "\n",
        "for doc in documents:\n",
        "    source = doc[\"source\"]\n",
        "    words = doc[\"content\"].split()\n",
        "\n",
        "    for i in range(0, len(words), MAX_CHUNK_WORDS - OVERLAP):\n",
        "        chunk_words = words[i:i + MAX_CHUNK_WORDS]\n",
        "        chunk_text = ' '.join(chunk_words)\n",
        "\n",
        "        chunks.append({\n",
        "            \"source\": source,\n",
        "            \"content\": chunk_text\n",
        "        })"
      ],
      "metadata": {
        "id": "guQ_fsUBvAlK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total de chunks: {len(chunks)}\")\n",
        "print(\"Ejemplo de chunk:\\n\", chunks[0][\"content\"][:700])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b60S3hnQw07a",
        "outputId": "4b9b268b-7d31-4be1-b03b-35f0c652b6c9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de chunks: 576\n",
            "Ejemplo de chunk:\n",
            " üìÑ Metadata de 170_1772666_Metadata_2025-04-07.csv: Name: Proportion of births attended by skilled health personnel (%) Short name: Proportion of births attended by skilled health personnel (%) Indicator unique identifier: 1772666 Indicator codes: MDG_0000000025 Also known as: SDG indicator 3.1.2 SDG Goal: 3.1.2 ‚Äì Births attended by skilled health personnel Short description: Proportion of births attended by skilled health personnel. (SDG 3.1.2) Definition: Proportion of births attended by skilled health personnel (generally doctors, nurses or midwives but can refer to other health professionals providing childbirth care) is the proportion of childbirths attended by professional health person\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorizaci√≥n (embeddings)\n",
        "Los modelos de lenguaje como los usados en RAG no buscan respuestas de forma exacta por coincidencia de palabras, sino que comparan significados. Para hacer esto, necesitamos representar cada fragmento de texto (chunk) como un vector en un espacio multidimensional. Luego podemos encontrar los textos m√°s relevantes comparando su distancia sem√°ntica.\n",
        "\n",
        "Utilizamos el modelo all-MiniLM-L6-v2 de la librer√≠a sentence-transformers, el cual transforma cada chunk de texto en un vector num√©rico de 384 dimensiones que resume su contenido de forma sem√°ntica. Luego extraemos los textos de los chunks previamente generados y los vectorizamos"
      ],
      "metadata": {
        "id": "l5y-4xAlxLrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el modelo\n",
        "#model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Extraer solo el texto de los chunks\n",
        "#texts = [chunk['content'] for chunk in chunks]\n",
        "\n",
        "# Vectorizar los textos\n",
        "#embeddings = model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "# Convertir a matriz numpy\n",
        "#embeddings_np = np.array(embeddings)"
      ],
      "metadata": {
        "id": "K9q0LLGCxUPx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "82e6c99c",
        "outputId": "441161e0-baa1-47b8-bd83-12f0dd4ec4fc"
      },
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "import pandas as pd\n",
        "import numpy as np # Asegurarse de importar numpy\n",
        "\n",
        "# Configura tu clave de API de Gemini.\n",
        "# Es mejor usar Colab Secrets para almacenar tu clave de API de forma segura.\n",
        "# Ve a \"üîë\" en el panel de la izquierda, agrega un nuevo secreto llamado \"GEMINI_API_KEY\"\n",
        "# y pega tu clave de API all√≠. Luego, usa la siguiente l√≠nea para acceder a ella:\n",
        "from google.colab import userdata\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "\n",
        "# Esta funci√≥n toma una oraci√≥n como argumento y devuelve sus embeddings\n",
        "def get_embeddings(text):\n",
        "    # Define the embedding model\n",
        "    model = 'models/embedding-001'\n",
        "    # Get the embeddings\n",
        "    embedding = genai.embed_content(model=model,\n",
        "                                    content=text,\n",
        "                                    task_type=\"retrieval_document\")\n",
        "    return embedding['embedding']\n",
        "\n",
        "# Aseg√∫rate de que 'chunks' est√© definido y contenga tus documentos (chunks)\n",
        "# Por ejemplo, si usaste la variable 'chunks' en celdas anteriores:\n",
        "if 'chunks' not in globals():\n",
        "    print(\"La variable 'chunks' no est√° definida. Aseg√∫rate de haber ejecutado las celdas anteriores para crear los chunks.\")\n",
        "else:\n",
        "    # Get the content from the chunks and create a new list\n",
        "    content_list = [chunk['content'] for chunk in chunks]\n",
        "\n",
        "    # Send one chunk content at a time to get embeddings\n",
        "    print(f\"Generando embeddings para {len(content_list)} chunks...\")\n",
        "    # Implementaci√≥n de un manejo de errores b√°sico y reintentos para la API de Gemini\n",
        "    embeddings = []\n",
        "    for i, content in enumerate(content_list):\n",
        "        try:\n",
        "            embeddings.append(get_embeddings(content))\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f\"Embeddings generados para {i + 1}/{len(content_list)} chunks.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error al generar embedding para chunk {i}: {e}. Saltando este chunk.\")\n",
        "            # Puedes optar por agregar un embedding de ceros o None si un chunk falla\n",
        "            # embeddings.append([0.0] * dimension) # Asumiendo que conoces la dimensi√≥n\n",
        "            # O simplemente saltarlo, lo que har√° que la lista de embeddings sea m√°s corta que content_list\n",
        "\n",
        "    print(\"Generaci√≥n de embeddings finalizada.\")\n",
        "\n",
        "    # Convertir a matriz numpy\n",
        "    # Asegurarse de que todos los embeddings tengan la misma longitud antes de convertir a numpy\n",
        "    if embeddings:\n",
        "        # Puedes verificar la longitud del primer embedding para la dimensi√≥n\n",
        "        # dimension = len(embeddings[0])\n",
        "        # O si usaste SentenceTransformer antes, podr√≠as usar la dimensi√≥n de ese modelo (768)\n",
        "        # dimension = 768 # O la dimensi√≥n correcta de 'models/embedding-001'\n",
        "\n",
        "        # Convertir a numpy array\n",
        "        embeddings_np_gemini = np.array(embeddings)\n",
        "        print(\"Embeddings de Gemini convertidos a numpy array.\")\n",
        "        print(\"Forma del array de embeddings de Gemini:\", embeddings_np_gemini.shape)\n",
        "\n",
        "        # Nota: Este array de embeddings (embeddings_np_gemini) es diferente al generado por SentenceTransformer.\n",
        "        # Deber√°s decidir cu√°l usar para construir tu √≠ndice FAISS.\n",
        "        # Si quieres usar estos embeddings de Gemini con el √≠ndice FAISS existente,\n",
        "        # necesitar√≠as crear un nuevo √≠ndice FAISS con la dimensi√≥n correcta para estos embeddings\n",
        "        # y a√±adir este nuevo array a ese √≠ndice.\n",
        "    else:\n",
        "        print(\"No se generaron embeddings.\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generando embeddings para 576 chunks...\n",
            "Embeddings generados para 10/576 chunks.\n",
            "Error al generar embedding para chunk 12: Failed to parse JSON: TypeError: 'NoneType' object is not iterable.. Saltando este chunk.\n",
            "Error al generar embedding para chunk 18: Failed to parse JSON: TypeError: 'NoneType' object is not iterable.. Saltando este chunk.\n",
            "Embeddings generados para 20/576 chunks.\n",
            "Embeddings generados para 30/576 chunks.\n",
            "Embeddings generados para 40/576 chunks.\n",
            "Embeddings generados para 50/576 chunks.\n",
            "Embeddings generados para 60/576 chunks.\n",
            "Embeddings generados para 70/576 chunks.\n",
            "Embeddings generados para 80/576 chunks.\n",
            "Embeddings generados para 90/576 chunks.\n",
            "Embeddings generados para 100/576 chunks.\n",
            "Embeddings generados para 110/576 chunks.\n",
            "Embeddings generados para 120/576 chunks.\n",
            "Embeddings generados para 130/576 chunks.\n",
            "Embeddings generados para 140/576 chunks.\n",
            "Embeddings generados para 150/576 chunks.\n",
            "Embeddings generados para 160/576 chunks.\n",
            "Embeddings generados para 170/576 chunks.\n",
            "Embeddings generados para 180/576 chunks.\n",
            "Embeddings generados para 190/576 chunks.\n",
            "Embeddings generados para 200/576 chunks.\n",
            "Embeddings generados para 210/576 chunks.\n",
            "Embeddings generados para 220/576 chunks.\n",
            "Embeddings generados para 230/576 chunks.\n",
            "Embeddings generados para 240/576 chunks.\n",
            "Embeddings generados para 250/576 chunks.\n",
            "Embeddings generados para 260/576 chunks.\n",
            "Embeddings generados para 270/576 chunks.\n",
            "Embeddings generados para 280/576 chunks.\n",
            "Embeddings generados para 290/576 chunks.\n",
            "Embeddings generados para 300/576 chunks.\n",
            "Embeddings generados para 310/576 chunks.\n",
            "Embeddings generados para 320/576 chunks.\n",
            "Embeddings generados para 330/576 chunks.\n",
            "Embeddings generados para 340/576 chunks.\n",
            "Embeddings generados para 350/576 chunks.\n",
            "Embeddings generados para 360/576 chunks.\n",
            "Embeddings generados para 370/576 chunks.\n",
            "Embeddings generados para 380/576 chunks.\n",
            "Embeddings generados para 390/576 chunks.\n",
            "Embeddings generados para 400/576 chunks.\n",
            "Embeddings generados para 410/576 chunks.\n",
            "Embeddings generados para 420/576 chunks.\n",
            "Embeddings generados para 430/576 chunks.\n",
            "Embeddings generados para 440/576 chunks.\n",
            "Embeddings generados para 450/576 chunks.\n",
            "Embeddings generados para 460/576 chunks.\n",
            "Embeddings generados para 470/576 chunks.\n",
            "Embeddings generados para 480/576 chunks.\n",
            "Embeddings generados para 490/576 chunks.\n",
            "Embeddings generados para 500/576 chunks.\n",
            "Embeddings generados para 510/576 chunks.\n",
            "Embeddings generados para 520/576 chunks.\n",
            "Embeddings generados para 530/576 chunks.\n",
            "Embeddings generados para 540/576 chunks.\n",
            "Embeddings generados para 550/576 chunks.\n",
            "Embeddings generados para 560/576 chunks.\n",
            "Embeddings generados para 570/576 chunks.\n",
            "Generaci√≥n de embeddings finalizada.\n",
            "Embeddings de Gemini convertidos a numpy array.\n",
            "Forma del array de embeddings de Gemini: (574, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos FAISS (Facebook AI Similarity Search) para almacenar los vectores y poder hacer b√∫squedas por similitud. Esto nos permite recuperar los chunks m√°s cercanos a una consulta (query) en t√©rminos de significado."
      ],
      "metadata": {
        "id": "Gn9rpB1v59TT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos un √≠ndice FAISS para b√∫squedas por similitud\n",
        "# La dimensi√≥n debe coincidir con la del array de embeddings de Gemini\n",
        "if 'embeddings_np_gemini' in globals() and embeddings_np_gemini.shape[0] > 0:\n",
        "    dimension = embeddings_np_gemini.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "    # A√±adir los vectores de Gemini\n",
        "    index.add(embeddings_np_gemini)\n",
        "    print(f\"√çndice FAISS creado con {index.ntotal} vectores de dimensi√≥n {dimension}.\")\n",
        "else:\n",
        "    print(\"No se encontraron embeddings de Gemini v√°lidos para crear el √≠ndice FAISS.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBEu3vJpxfpt",
        "outputId": "b5fbae0a-85c2-4fdd-ee32-84081ac7b13d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "√çndice FAISS creado con 574 vectores de dimensi√≥n 768.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar los metadatos (puede ser √∫til para devolver respuestas)\n",
        "# Asegurarse de que la lista de chunks sea coherente con los embeddings generados\n",
        "if 'chunks' in globals() and 'embeddings_np_gemini' in globals() and len(chunks) >= embeddings_np_gemini.shape[0]:\n",
        "    # Si algunos chunks fallaron al generar embeddings, solo guarda los metadatos para los chunks exitosos\n",
        "    # Una forma simple es asumir que los primeros N chunks corresponden a los N embeddings exitosos\n",
        "    # Sin embargo, si hubo saltos (como en la salida anterior), la correspondencia directa puede ser incorrecta.\n",
        "    # Un enfoque m√°s robusto ser√≠a mantener un registro de qu√© chunks generaron embeddings exitosamente.\n",
        "    # Por ahora, asumiremos que el orden se mantuvo para los chunks exitosos y truncamos si es necesario.\n",
        "    chunk_sources = [chunk[\"source\"] for chunk in chunks[:embeddings_np_gemini.shape[0]]]\n",
        "    print(f\"Guardados metadatos para {len(chunk_sources)} chunks correspondientes a los embeddings generados.\")\n",
        "elif 'chunks' in globals():\n",
        "     print(\"Advertencia: El n√∫mero de chunks no coincide con el n√∫mero de embeddings generados.\")\n",
        "     chunk_sources = [chunk[\"source\"] for chunk in chunks]\n",
        "else:\n",
        "     print(\"No se encontraron chunks para guardar metadatos.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgyMcJ8cxkn8",
        "outputId": "4f060be9-5610-4fd7-e87c-56da6494f157"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Guardados metadatos para 574 chunks correspondientes a los embeddings generados.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con el √≠ndice creado, ahora podemos consultar el sistema en lenguaje natural. FAISS devuelve los chunks m√°s relevantes con base en la similitud sem√°ntica entre la consulta y los documentos vectorizados."
      ],
      "metadata": {
        "id": "0sBp-G4C6CJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de b√∫squeda\n",
        "query = \"malaria\"\n",
        "# Vectorizar la consulta usando la funci√≥n get_embeddings\n",
        "query_vec = get_embeddings(query)\n",
        "\n",
        "# Buscar los 5 chunks m√°s similares. index.search espera numpy arrays.\n",
        "# get_embeddings devuelve una lista, la convertimos a numpy array y ajustamos la forma si es necesario\n",
        "query_vec_np = np.array([query_vec])\n",
        "\n",
        "\n",
        "distances, indices = index.search(query_vec_np, k=5)\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(f\"Resultados de la b√∫squeda para '{query}':\")\n",
        "# Asegurarse de tener chunk_sources disponibles\n",
        "if 'chunk_sources' in globals() and len(chunk_sources) > 0:\n",
        "    for i in indices[0]:\n",
        "        # Asegurarse de que el √≠ndice est√© dentro del rango de chunks y chunk_sources\n",
        "        if 0 <= i < len(chunks) and 0 <= i < len(chunk_sources):\n",
        "            print(\"üîπ Source:\", chunk_sources[i]) # Usar chunk_sources para el nombre del archivo\n",
        "            # Limitar la salida para mayor legibilidad\n",
        "            print(chunks[i][\"content\"][:500] + \"...\")\n",
        "            print(\"-\" * 80)\n",
        "        else:\n",
        "            print(f\"‚ö†Ô∏è √çndice fuera de rango v√°lido para chunks o chunk_sources: {i}\") # Manejar √≠ndices inv√°lidos\n",
        "else:\n",
        "    print(\"No se encontraron metadatos de chunks (chunk_sources) para mostrar los resultados.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "CP3OCKuYxmcB",
        "outputId": "404891ac-2fd4-4fd7-d212-dba934a653d7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados de la b√∫squeda para 'malaria':\n",
            "üîπ Source: 2D6FBE4_3.3.5 - Neglected tropical diseases/170_2D6FBE4_Metadata_2024-01-08.csv\n",
            "a subset of the larger number of people at risk. Mass treatment is limited to those living in districts above a threshold level of prevalence; it does not include all people living in districts with any risk of infection. Individual treatment and care is for those who are or have already been infected; it does not include all contacts and others at risk of infection. This number can better be interpreted as the number of people at a level of risk requiring medical intervention ‚Äì that is, treatme...\n",
            "--------------------------------------------------------------------------------\n",
            "üîπ Source: 442CEA8_3.3.3 - Malaria/170_442CEA8_Dataset_2024-03-08.csv\n",
            "to DIM_TIME.: YEAR | Temporal dimension value (e.g. 2020). Related to DIM_TIME_TYPE.: 2016 | RATE_PER_1000_N: 10.95893 | RATE_PER_1000_NL: 8.33429 | RATE_PER_1000_NU: 13.66553 Indicator Unique ID (seven-digit UID).: 442CEA8 | Full name of Indicator.: Malaria cases | Geographic dimension code type (e.g. \"Country\", \"Region\", \"Global\"). Related to DIM_GEO_CODE_M49.: COUNTRY | Geographic dimension code using M49 SDMX standard (e.g. 004). Related to DIM_GEO_CODE_TYPE.: 170 | Geographic dimension shor...\n",
            "--------------------------------------------------------------------------------\n",
            "üîπ Source: 45CA7C8_3.C.1 - Health worker density and distribution/170_45CA7C8_Metadata_2024-05-20.csv\n",
            "190 Geographic entities and groups: Afghanistan, Albania, Algeria, Andorra, Angola, Antigua and Barbuda, Argentina, Armenia, Australia, Austria, Azerbaijan, Bahamas, Bahrain, Bangladesh, Barbados, Belarus, Belgium, Belize, Benin, Bhutan, Bolivia (Plurinational State of), Bosnia and Herzegovina, Botswana, Brazil, Brunei Darussalam, Bulgaria, Burkina Faso, Burundi, Cabo Verde, Cambodia, Cameroon, Canada, Central African Republic, Chad, Chile, China, Colombia, Comoros, Congo, Cook Islands, Costa Ri...\n",
            "--------------------------------------------------------------------------------\n",
            "üîπ Source: 1F96863_3.4.1 - cardiovascular disease, cancer, diabetes or chronic respiratory disease/170_1F96863_Metadata_2024-12-18.csv\n",
            "surveys with verbal autopsy, and sample or sentinel registration systems. Expected frequency of data collection: Annual Data providers: National statistics offices and/or ministries of health. Data compilers: World Health Organization (WHO) üßæ Metadata extra√≠da de 170_1F96863_Metadata_2024-12-18.csv: Name: Probability of premature mortality from NCDs Short name: Probability of dying from any of CVD, cancer, diabetes, CRD between age 30 and exact age 70 (%) Indicator unique identifier: 1F96863 Ind...\n",
            "--------------------------------------------------------------------------------\n",
            "üîπ Source: ED50112_3.9.2 - Unsafe water, unsafe sanitation and lack of hygiene/170_ED50112_Metadata_2024-01-08.csv\n",
            "A06-A09), intestinal nematode infections (ICD-10 code B76-B77, B79), protein-energy malnutrition (ICD-10 code E40-E46) and acute respiratory infections (ICD-10 codes H65-H66, J00-J22, P23, and U04). Direction of progress: Figures decreasing over time demonstrates progress. Framework classifications: Impact Indicator status: Actively collected and reported Most recent data update: 8 January 2024 Publishing institution: WHO Data type: Float Unit of measure: Number of deaths attributable to unsafe ...\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/@myscale/how-to-build-a-rag-powered-chatbot-with-google-gemini-and-myscaledb-79c0024cd237"
      ],
      "metadata": {
        "id": "rp8HSJhSwXf9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55122c89"
      },
      "source": [
        "# Configuraci√≥n del modelo generativo y Generaci√≥n de respuesta\n",
        "\n",
        "Ahora que podemos recuperar informaci√≥n relevante de nuestra base de datos vectorial, el siguiente paso es utilizar un modelo de lenguaje grande (LLM) para generar una respuesta basada en la consulta del usuario y la informaci√≥n recuperada.\n",
        "\n",
        "Utilizaremos el modelo `gemini-pro` para esta tarea.\n",
        "\n",
        "El proceso general ser√°:\n",
        "\n",
        "1.  El usuario hace una consulta.\n",
        "2.  Vectorizamos la consulta usando la funci√≥n `get_embeddings`.\n",
        "3.  Buscamos los chunks m√°s relevantes en el √≠ndice FAISS usando el vector de la consulta.\n",
        "4.  Tomamos el contenido de los chunks recuperados.\n",
        "5.  Enviamos la consulta original del usuario y el contenido de los chunks recuperados al modelo `gemini-pro` con una instrucci√≥n (prompt) para que genere una respuesta informada por esos documentos.\n",
        "6.  Presentamos la respuesta generada al usuario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "5f96c266",
        "outputId": "4f782d40-0e13-4efb-e255-a1f08a1ad0f9"
      },
      "source": [
        "# Configurar el modelo generativo Gemini\n",
        "\n",
        "model_generative = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "\n",
        "\n",
        "def generate_rag_response(query, retrieved_chunks):\n",
        "    \"\"\"\n",
        "    Genera una respuesta a partir de una consulta y los chunks m√°s relevantes,\n",
        "    usando el modelo generativo Gemini.\n",
        "\n",
        "    Args:\n",
        "        query (str): La pregunta del usuario.\n",
        "        retrieved_chunks (list): Lista de diccionarios, cada uno con la clave 'content'.\n",
        "\n",
        "    Returns:\n",
        "        str: Respuesta generada por Gemini basada en el contexto.\n",
        "    \"\"\"\n",
        "    # Combinar chunks en un √∫nico contexto con separaci√≥n visual\n",
        "    context = \"\\n---\\n\".join([chunk['content'] for chunk in retrieved_chunks])\n",
        "\n",
        "    # Crear prompt para el modelo generativo\n",
        "    prompt = f\"\"\"Usa la siguiente informaci√≥n para responder la pregunta.\n",
        "\n",
        "Contexto:\n",
        "{context}\n",
        "\n",
        "Pregunta:\n",
        "{query}\n",
        "\n",
        "Si no encuentras suficiente informaci√≥n en el contexto, responde que no tienes datos relevantes sobre el tema.\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = model_generative.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\" Error al generar respuesta con Gemini: {e}\")\n",
        "        return \"Lo siento, no pude generar una respuesta en este momento.\"\n",
        "\n",
        "\n",
        "# ==========\n",
        "# FLUJO DE B√öSQUEDA RAG\n",
        "# ==========\n",
        "\n",
        "query = \"Violencia sexual en Colombia\"\n",
        "\n",
        "if 'get_embeddings' in globals() and 'index' in globals():\n",
        "    try:\n",
        "        # Paso 1: Vectorizar consulta\n",
        "        query_vec = get_embeddings(query)\n",
        "        query_vec_np = np.array([query_vec])\n",
        "\n",
        "        # Paso 2: Buscar los k chunks m√°s relevantes\n",
        "        distances, indices = index.search(query_vec_np, k=5)\n",
        "\n",
        "        # Paso 3: Recuperar contenido de los chunks\n",
        "        retrieved_chunks = [chunks[i] for i in indices[0] if 0 <= i < len(chunks)]\n",
        "\n",
        "        # Paso 4: Generar respuesta\n",
        "        if retrieved_chunks:\n",
        "            print(\"\\n Generando respuesta RAG para tu consulta...\")\n",
        "            rag_response = generate_rag_response(query, retrieved_chunks)\n",
        "            print(\"\\n Respuesta del Chatbot:\\n\")\n",
        "            print(rag_response)\n",
        "        else:\n",
        "            print(\"No se encontraron chunks relevantes.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error durante el proceso de b√∫squeda RAG: {e}\")\n",
        "else:\n",
        "    print(\" 'get_embeddings' o 'index' no est√°n definidos.\")\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Generando respuesta RAG para tu consulta...\n",
            "\n",
            " Respuesta del Chatbot:\n",
            "\n",
            "No tengo datos relevantes sobre violencia sexual en Colombia en el texto proporcionado.  El texto incluye informaci√≥n sobre enfermedades tropicales desatendidas, homicidios y contaminaci√≥n del aire, pero no aborda la violencia sexual.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}