{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO8gfFa3mdhl6uZ5vaMEdmK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/valeromora/TAM_2025-1/blob/main/Final%20Proyect/Final_proyect_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Set up"
      ],
      "metadata": {
        "id": "Ofgbef-NapRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries if they are not already installed\n",
        "!pip install PyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbFQJK-8pSvU",
        "outputId": "5724168a-ae32-49a2-8be7-ee64b60d8197"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.11/dist-packages (1.3.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.11/dist-packages (from PyDrive) (2.174.0)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.11/dist-packages (from PyDrive) (6.0.2)\n",
            "Requirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.2->PyDrive) (2.38.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.2->PyDrive) (2.25.1)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client>=1.2->PyDrive) (4.2.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.2)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.9.1)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from oauth2client>=4.0.0->PyDrive) (1.17.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (1.70.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (5.29.5)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (1.26.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client>=1.2->PyDrive) (5.5.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client>=1.2->PyDrive) (3.2.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client>=1.2->PyDrive) (2025.6.15)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhRt8kx9wIvt",
        "outputId": "75772734-99f1-4bfb-cfb7-ad31f05c7eab"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8kOgxSDHxV78",
        "outputId": "65aa554c-9d8b-4cea-f001-dc781cf699c0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-5.0.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.53.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.6.15)\n",
            "Downloading sentence_transformers-5.0.0-py3-none-any.whl (470 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m470.2/470.2 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, faiss-cpu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: sentence-transformers\n",
            "    Found existing installation: sentence-transformers 4.1.0\n",
            "    Uninstalling sentence-transformers-4.1.0:\n",
            "      Successfully uninstalled sentence-transformers-4.1.0\n",
            "Successfully installed faiss-cpu-1.11.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 sentence-transformers-5.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbT3_kKcamUD",
        "outputId": "29831739-1b44-4d56-8a74-ac66dd79c2e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Database"
      ],
      "metadata": {
        "id": "Nsck3Q8wIE44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://drive.google.com/file/d/1x6HASgMM9WqaglFR-zH2_LU6eRs4l_tq/view?usp=sharing\n",
        "\n",
        "# Authenticate and create the PyDrive client\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Download the file\n",
        "file_id = '1x6HASgMM9WqaglFR-zH2_LU6eRs4l_tq'\n",
        "downloaded = drive.CreateFile({'id': file_id})\n",
        "downloaded.GetContentFile('downloaded_file.zip')\n",
        "\n",
        "# Unzip the file\n",
        "with zipfile.ZipFile('downloaded_file.zip', 'r') as zip_ref:\n",
        "  zip_ref.extractall('/content/')\n",
        "\n",
        "# List the extracted files to confirm\n",
        "!ls /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObIrVAqIbUp5",
        "outputId": "4ba9d014-dd7f-4b62-fb69-54f90bd4664c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'12EE54A_6.2.1 - Safely managed sanitation & hand-washing'\n",
            "'1548EA3_6.1.1 - Safely managed drinking water'\n",
            "'16BBF41_3.4.2 - Suicide'\n",
            "'1772666_3.1.2 - Births attended by skilled health personnel'\n",
            "'1F96863_3.4.1 - cardiovascular disease, cancer, diabetes or chronic respiratory disease'\n",
            "'217795A_3.C.1 - Health worker density and distribution'\n",
            "'2322814_3.2.1 - Under-five mortality rate'\n",
            "'2D6FBE4_3.3.5 - Neglected tropical diseases'\n",
            "'361734E_16.1.1 - Intentional homicide'\n",
            "'442CEA8_3.3.3 - Malaria'\n",
            "'45CA7C8_3.C.1 - Health worker density and distribution'\n",
            "'5C8435F_3.C.1 - Health worker density and distribution'\n",
            "'5F8A486_2.2.1 - Stunting'\n",
            " 608DE39\n",
            "'6A64C9A_7.1.2 - Clean fuels'\n",
            "'75DDA77_3.A.1 - Age-standardized prevalence of tobacco use'\n",
            "'77D059C_3.3.1 - HIV infections'\n",
            "'8074BD9_3.7.1 - Women satisfied with modern methods'\n",
            "'84FD3DE_3.9.3 - Unintentional poisoning'\n",
            "'A37BDD6_6.3.1 - Safely treated wastewater'\n",
            "'A4C49D3_3.2.2- Neonatal mortality rate'\n",
            "'AC597B1_3.1.1 - Maternal mortality ratio'\n",
            "'B9C6C79_1.a.2 - Government spending essential services'\n",
            "'BBF3A64_3.B.2 - Development assistance'\n",
            " BEFA58B\n",
            "'C288D13_3.3.2 - Tuberculosis'\n",
            "'D1223E8_6.2.1 - Safely managed sanitation & hand-washing'\n",
            "'D6176E2_3.6.1 - Deaths due to road traffic injuries'\n",
            " downloaded_file.zip\n",
            "'E0D4E17_5.2.2 - Sexual violence by persons other than an intimate partner (last 12 months)'\n",
            "'E2FC6D7_3.9.1 - Household and ambient air pollution'\n",
            "'ED50112_3.9.2 - Unsafe water, unsafe sanitation and lack of hygiene'\n",
            "'EE6F72A_3.5.2 - Alcohol'\n",
            " EF93DDB\n",
            "'F513188_3.3.4 - Hepatitis B'\n",
            "'F810947_11.6.2 - Fine particulate matter'\n",
            "'F8524F2_5.2.1 - Intimate partner violence (last 12 months)'\n",
            "'F8E084C_3.B.1 - Access to affordable medicines and vaccines'\n",
            "'FC5231F_2.2.2 - Malnutrition'\n",
            " sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparación archivos\n",
        "### Extracción y enriquecimiento de información por carpeta\n",
        "Este bloque de código recorre cada carpeta del dataset, busca archivos relevantes (`Metadata`, `Dictionary`, `Code list`, `Dataset`) y construye documentos enriquecidos en texto plano.\n",
        "\n",
        "- Se extraen definiciones de variables desde el archivo **Data Dictionary**.\n",
        "- Se interpretan valores codificados usando el **Code List** (si está presente).\n",
        "- Se incluye el contexto general desde el archivo **Metadata**.\n",
        "- Cada fila del Dataset es enriquecida con descripciones legibles para facilitar su comprensión y futura vectorización.\n",
        "- El resultado se almacena en una lista de diccionarios llamada `documents`, lista para dividirse en _chunks_.\n",
        "\n",
        "Esto permite aprovechar mejor los datos, incluso si vienen organizados de manera heterogénea.\n"
      ],
      "metadata": {
        "id": "CAV7g8tRusfl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = \"/content\"\n",
        "csv_count = 0\n",
        "folders_with_csv = {}\n",
        "\n",
        "for folder in os.listdir(base_path):\n",
        "    folder_path = os.path.join(base_path, folder)\n",
        "    if os.path.isdir(folder_path):\n",
        "        csv_files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
        "        csv_count += len(csv_files)\n",
        "        folders_with_csv[folder] = csv_files\n",
        "\n",
        "print(f\"Total de archivos .csv encontrados: {csv_count}\")\n",
        "print(f\"Resumen por carpeta:\")\n",
        "for folder, files in folders_with_csv.items():\n",
        "    print(f\"{folder}: {len(files)} archivos\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlX54D2z_s6H",
        "outputId": "d2eb88ee-f941-4649-9f56-520ed7a8e46d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de archivos .csv encontrados: 139\n",
            "Resumen por carpeta:\n",
            ".config: 0 archivos\n",
            "1772666_3.1.2 - Births attended by skilled health personnel: 3 archivos\n",
            "F810947_11.6.2 - Fine particulate matter: 4 archivos\n",
            "AC597B1_3.1.1 - Maternal mortality ratio: 3 archivos\n",
            "84FD3DE_3.9.3 - Unintentional poisoning: 4 archivos\n",
            "BEFA58B: 4 archivos\n",
            "F513188_3.3.4 - Hepatitis B: 3 archivos\n",
            "B9C6C79_1.a.2 - Government spending essential services: 3 archivos\n",
            "12EE54A_6.2.1 - Safely managed sanitation & hand-washing: 4 archivos\n",
            "2322814_3.2.1 - Under-five mortality rate: 4 archivos\n",
            "361734E_16.1.1 - Intentional homicide: 4 archivos\n",
            "D1223E8_6.2.1 - Safely managed sanitation & hand-washing: 4 archivos\n",
            "6A64C9A_7.1.2 - Clean fuels: 4 archivos\n",
            "EF93DDB: 4 archivos\n",
            "5F8A486_2.2.1 - Stunting: 3 archivos\n",
            "C288D13_3.3.2 - Tuberculosis: 3 archivos\n",
            "A37BDD6_6.3.1 - Safely treated wastewater: 3 archivos\n",
            "A4C49D3_3.2.2- Neonatal mortality rate: 4 archivos\n",
            "F8524F2_5.2.1 - Intimate partner violence (last 12 months): 3 archivos\n",
            "D6176E2_3.6.1 - Deaths due to road traffic injuries: 3 archivos\n",
            "EE6F72A_3.5.2 - Alcohol: 4 archivos\n",
            "BBF3A64_3.B.2 - Development assistance: 3 archivos\n",
            "ED50112_3.9.2 - Unsafe water, unsafe sanitation and lack of hygiene: 4 archivos\n",
            "75DDA77_3.A.1 - Age-standardized prevalence of tobacco use: 4 archivos\n",
            "77D059C_3.3.1 - HIV infections: 4 archivos\n",
            "16BBF41_3.4.2 - Suicide: 4 archivos\n",
            "2D6FBE4_3.3.5 - Neglected tropical diseases: 3 archivos\n",
            "1548EA3_6.1.1 - Safely managed drinking water: 4 archivos\n",
            "442CEA8_3.3.3 - Malaria: 3 archivos\n",
            "608DE39: 4 archivos\n",
            "FC5231F_2.2.2 - Malnutrition: 4 archivos\n",
            "5C8435F_3.C.1 - Health worker density and distribution: 3 archivos\n",
            "217795A_3.C.1 - Health worker density and distribution: 3 archivos\n",
            "45CA7C8_3.C.1 - Health worker density and distribution: 3 archivos\n",
            "1F96863_3.4.1 - cardiovascular disease, cancer, diabetes or chronic respiratory disease: 4 archivos\n",
            "E0D4E17_5.2.2 - Sexual violence by persons other than an intimate partner (last 12 months): 3 archivos\n",
            "F8E084C_3.B.1 - Access to affordable medicines and vaccines: 3 archivos\n",
            "E2FC6D7_3.9.1 - Household and ambient air pollution: 4 archivos\n",
            "8074BD9_3.7.1 - Women satisfied with modern methods: 4 archivos\n",
            "sample_data: 4 archivos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ruta base donde se encuentran las carpetas con los datasets y archivos relacionados\n",
        "base_path = \"/content\"\n",
        "\n",
        "# Lista que almacenará los documentos generados por cada carpeta,\n",
        "# donde cada documento es un texto enriquecido con metadata, definiciones y datos\n",
        "documents = []\n",
        "\n",
        "# Recorremos cada carpeta dentro de la ruta base\n",
        "for folder in os.listdir(base_path):\n",
        "    folder_path = os.path.join(base_path, folder)\n",
        "    # Saltamos si no es carpeta (p.ej., archivos sueltos)\n",
        "    if not os.path.isdir(folder_path):\n",
        "        continue\n",
        "\n",
        "    # Diccionarios de mapeo para traducir y dar contexto:\n",
        "    # data_dict_map: mapea nombre de variable → definición textual\n",
        "    data_dict_map = {}\n",
        "    # code_list_map: mapea (dimensión, código) → descripción del código\n",
        "    code_list_map = {}\n",
        "\n",
        "    # Variable para acumular el texto de metadata (solo un archivo por carpeta)\n",
        "    metadata_text = \"\"\n",
        "\n",
        "    # Búsqueda y extracción del archivo de Metadata para la carpeta actual\n",
        "    # Solo se procesa el primer archivo que cumpla la condición\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if \"Metadata\" in filename and filename.endswith(\".csv\"):\n",
        "            try:\n",
        "                df_meta = pd.read_csv(os.path.join(folder_path, filename))\n",
        "                metadata_text += f\"📄 Metadata de {filename}:\\n\"\n",
        "                # Recorremos las filas sin índice para extraer clave y valor\n",
        "                for row in df_meta.itertuples(index=False):\n",
        "                    if len(row) >= 2:\n",
        "                        key = str(row[0]).strip()\n",
        "                        value = str(row[1]).strip()\n",
        "                        metadata_text += f\"{key}: {value}\\n\"\n",
        "            except Exception as e:\n",
        "                print(f\"No se pudo leer metadata en {folder}: {e}\")\n",
        "            break  # Solo procesamos un archivo Metadata por carpeta\n",
        "\n",
        "    # Primera pasada: cargamos diccionarios para darle sentido a los datos\n",
        "    for filename in os.listdir(folder_path):\n",
        "        # Procesamos solo archivos CSV\n",
        "        if not filename.endswith(\".csv\"):\n",
        "            continue\n",
        "\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "        except Exception as e:\n",
        "            print(f\"No se pudo leer {filename}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Si es un archivo Data Dictionary, cargamos definiciones de variables\n",
        "        if \"Dictionary\" in filename or \"Data Dictionary\" in filename:\n",
        "            for row in df.itertuples(index=False):\n",
        "                if len(row) >= 2:\n",
        "                    var = str(row[0]).strip()\n",
        "                    definition = str(row[1]).strip()\n",
        "                    # Guardamos mapeo: nombre_variable -> definición\n",
        "                    data_dict_map[var] = definition\n",
        "\n",
        "        # Si es un archivo Code List, cargamos descripciones de códigos por dimensión\n",
        "        elif \"Code list\" in filename:\n",
        "            for row in df.itertuples(index=False):\n",
        "                if len(row) >= 4:\n",
        "                    dim = str(row[0]).strip()     # Dimensión o categoría\n",
        "                    key = str(row[1]).strip()     # Código o clave\n",
        "                    name = str(row[2]).strip()    # Nombre o etiqueta\n",
        "                    desc = str(row[3]).strip()    # Descripción detallada\n",
        "                    # Guardamos mapeo: (dimensión, código) -> descripción enriquecida\n",
        "                    code_list_map[(dim, key)] = f\"{name}: {desc}\"\n",
        "\n",
        "    # Segunda pasada: procesamos archivos que contengan los datos en sí\n",
        "    for filename in os.listdir(folder_path):\n",
        "        # Saltamos si no es CSV o si es Dictionary o Code list (ya procesados)\n",
        "        if not filename.endswith(\".csv\") or \"Dictionary\" in filename or \"Code list\" in filename:\n",
        "            continue\n",
        "\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        try:\n",
        "            df = pd.read_csv(file_path)\n",
        "        except Exception as e:\n",
        "            print(f\"No se pudo leer {filename}: {e}\")\n",
        "            continue\n",
        "\n",
        "        text_content = \"\"\n",
        "\n",
        "        # Si por algún motivo volvemos a encontrar metadata, la agregamos\n",
        "        if \"Metadata\" in filename:\n",
        "            text_content += f\"🧾 Metadata extraída de {filename}:\\n\"\n",
        "            for row in df.itertuples(index=False):\n",
        "                if len(row) >= 2:\n",
        "                    key = str(row[0]).strip()\n",
        "                    value = str(row[1]).strip()\n",
        "                    text_content += f\"{key}: {value}\\n\"\n",
        "\n",
        "        # Si es un dataset con datos, construimos textos enriquecidos\n",
        "        elif \"Dataset\" in filename:\n",
        "            text_content += f\"📊 Datos enriquecidos de {filename}:\\n\"\n",
        "            # Recorremos fila por fila\n",
        "            for row in df.itertuples(index=False):\n",
        "                row_description = []\n",
        "                # Para cada columna, buscamos su definición y posible descripción de valor\n",
        "                for col in df.columns:\n",
        "                    # Valor en la celda actual\n",
        "                    value = getattr(row, col, \"\")\n",
        "                    # Nombre legible para la variable, si existe\n",
        "                    label = data_dict_map.get(col, col)\n",
        "\n",
        "                    # Si el valor es una cadena y existe descripción en code_list para esa dimensión y valor\n",
        "                    if isinstance(value, str) and (col, value) in code_list_map:\n",
        "                        value_desc = code_list_map[(col, value)]\n",
        "                        row_description.append(f\"{label}: {value_desc}\")\n",
        "                    else:\n",
        "                        # Si no hay descripción especial, usamos el valor tal cual\n",
        "                        row_description.append(f\"{label}: {value}\")\n",
        "\n",
        "                # Concatenamos la descripción enriquecida para la fila completa\n",
        "                text_content += \" | \".join(row_description) + \"\\n\"\n",
        "\n",
        "        else:\n",
        "            # Archivos no reconocidos se reportan para control\n",
        "            print(f\"Archivo sin tipo conocido: {filename}\")\n",
        "            continue\n",
        "\n",
        "        # Finalmente, agregamos a la lista de documentos con su fuente y contenido enriquecido\n",
        "        documents.append({\n",
        "            \"source\": f\"{folder}/{filename}\",\n",
        "            \"content\": metadata_text + \"\\n\" + text_content\n",
        "        })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjxNZePbpPf-",
        "outputId": "912b6dac-899d-4240-e928-a53aad489227"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivo sin tipo conocido: mnist_test.csv\n",
            "Archivo sin tipo conocido: california_housing_train.csv\n",
            "Archivo sin tipo conocido: california_housing_test.csv\n",
            "Archivo sin tipo conocido: mnist_train_small.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total documentos extraídos: {len(documents)}\")\n",
        "print(documents[0]['source'])\n",
        "print(documents[0]['content'][:1000])  # muestra los primeros caracteres del primer documento"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vrY33mO9uVuc",
        "outputId": "709a3496-506c-435e-d7a2-bf7d44fadca9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total documentos extraídos: 76\n",
            "1772666_3.1.2 - Births attended by skilled health personnel/170_1772666_Metadata_2025-04-07.csv\n",
            "📄 Metadata de 170_1772666_Metadata_2025-04-07.csv:\n",
            "Name: Proportion of births attended by skilled health personnel (%)\n",
            "Short name: Proportion of births attended by skilled health personnel (%)\n",
            "Indicator unique identifier: 1772666\n",
            "Indicator codes: MDG_0000000025\n",
            "Also known as: SDG indicator 3.1.2\n",
            "SDG Goal: 3.1.2 – Births attended by skilled health personnel\n",
            "Short description: Proportion of births attended by skilled health personnel.\r\n",
            "(SDG 3.1.2)\n",
            "Definition: Proportion of births attended by skilled health personnel (generally doctors, nurses or midwives but can refer to other health professionals providing childbirth care) is the proportion of childbirths attended by professional health personnel.\r\n",
            "According to the current definition (1) these are competent maternal and newborn health (MNH) professionals educated, trained and regulated to national and international standards. They are competent to: (i) provide and promote evidence-based, human-rights based, quality, socio-culturally sen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chunking\n",
        "\n",
        "Se busca dividir cada content del documento en trozos más pequeños.\n",
        "* Divide el texto en grupos de 500 palabras\n",
        "\n",
        "* Repite las últimas 50 palabras del chunk anterior (para mantener contexto)"
      ],
      "metadata": {
        "id": "ZDYSuRRVvk7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_CHUNK_WORDS = 500\n",
        "OVERLAP = 50\n",
        "\n",
        "chunks = []\n",
        "\n",
        "for doc in documents:\n",
        "    source = doc[\"source\"]\n",
        "    words = doc[\"content\"].split()\n",
        "\n",
        "    for i in range(0, len(words), MAX_CHUNK_WORDS - OVERLAP):\n",
        "        chunk_words = words[i:i + MAX_CHUNK_WORDS]\n",
        "        chunk_text = ' '.join(chunk_words)\n",
        "\n",
        "        chunks.append({\n",
        "            \"source\": source,\n",
        "            \"content\": chunk_text\n",
        "        })"
      ],
      "metadata": {
        "id": "guQ_fsUBvAlK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Total de chunks: {len(chunks)}\")\n",
        "print(\"Ejemplo de chunk:\\n\", chunks[0][\"content\"][:700])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b60S3hnQw07a",
        "outputId": "4b9b268b-7d31-4be1-b03b-35f0c652b6c9"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de chunks: 576\n",
            "Ejemplo de chunk:\n",
            " 📄 Metadata de 170_1772666_Metadata_2025-04-07.csv: Name: Proportion of births attended by skilled health personnel (%) Short name: Proportion of births attended by skilled health personnel (%) Indicator unique identifier: 1772666 Indicator codes: MDG_0000000025 Also known as: SDG indicator 3.1.2 SDG Goal: 3.1.2 – Births attended by skilled health personnel Short description: Proportion of births attended by skilled health personnel. (SDG 3.1.2) Definition: Proportion of births attended by skilled health personnel (generally doctors, nurses or midwives but can refer to other health professionals providing childbirth care) is the proportion of childbirths attended by professional health person\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorización (embeddings)\n",
        "Los modelos de lenguaje como los usados en RAG no buscan respuestas de forma exacta por coincidencia de palabras, sino que comparan significados. Para hacer esto, necesitamos representar cada fragmento de texto (chunk) como un vector en un espacio multidimensional. Luego podemos encontrar los textos más relevantes comparando su distancia semántica.\n",
        "\n",
        "Utilizamos el modelo all-MiniLM-L6-v2 de la librería sentence-transformers, el cual transforma cada chunk de texto en un vector numérico de 384 dimensiones que resume su contenido de forma semántica. Luego extraemos los textos de los chunks previamente generados y los vectorizamos"
      ],
      "metadata": {
        "id": "l5y-4xAlxLrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar el modelo\n",
        "#model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Extraer solo el texto de los chunks\n",
        "#texts = [chunk['content'] for chunk in chunks]\n",
        "\n",
        "# Vectorizar los textos\n",
        "#embeddings = model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "# Convertir a matriz numpy\n",
        "#embeddings_np = np.array(embeddings)"
      ],
      "metadata": {
        "id": "K9q0LLGCxUPx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "82e6c99c",
        "outputId": "441161e0-baa1-47b8-bd83-12f0dd4ec4fc"
      },
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "import pandas as pd\n",
        "import numpy as np # Asegurarse de importar numpy\n",
        "\n",
        "# Configura tu clave de API de Gemini.\n",
        "# Es mejor usar Colab Secrets para almacenar tu clave de API de forma segura.\n",
        "# Ve a \"🔑\" en el panel de la izquierda, agrega un nuevo secreto llamado \"GEMINI_API_KEY\"\n",
        "# y pega tu clave de API allí. Luego, usa la siguiente línea para acceder a ella:\n",
        "from google.colab import userdata\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get('GEMINI_API_KEY')\n",
        "\n",
        "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
        "\n",
        "# Esta función toma una oración como argumento y devuelve sus embeddings\n",
        "def get_embeddings(text):\n",
        "    # Define the embedding model\n",
        "    model = 'models/embedding-001'\n",
        "    # Get the embeddings\n",
        "    embedding = genai.embed_content(model=model,\n",
        "                                    content=text,\n",
        "                                    task_type=\"retrieval_document\")\n",
        "    return embedding['embedding']\n",
        "\n",
        "# Asegúrate de que 'chunks' esté definido y contenga tus documentos (chunks)\n",
        "# Por ejemplo, si usaste la variable 'chunks' en celdas anteriores:\n",
        "if 'chunks' not in globals():\n",
        "    print(\"La variable 'chunks' no está definida. Asegúrate de haber ejecutado las celdas anteriores para crear los chunks.\")\n",
        "else:\n",
        "    # Get the content from the chunks and create a new list\n",
        "    content_list = [chunk['content'] for chunk in chunks]\n",
        "\n",
        "    # Send one chunk content at a time to get embeddings\n",
        "    print(f\"Generando embeddings para {len(content_list)} chunks...\")\n",
        "    # Implementación de un manejo de errores básico y reintentos para la API de Gemini\n",
        "    embeddings = []\n",
        "    for i, content in enumerate(content_list):\n",
        "        try:\n",
        "            embeddings.append(get_embeddings(content))\n",
        "            if (i + 1) % 10 == 0:\n",
        "                print(f\"Embeddings generados para {i + 1}/{len(content_list)} chunks.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error al generar embedding para chunk {i}: {e}. Saltando este chunk.\")\n",
        "            # Puedes optar por agregar un embedding de ceros o None si un chunk falla\n",
        "            # embeddings.append([0.0] * dimension) # Asumiendo que conoces la dimensión\n",
        "            # O simplemente saltarlo, lo que hará que la lista de embeddings sea más corta que content_list\n",
        "\n",
        "    print(\"Generación de embeddings finalizada.\")\n",
        "\n",
        "    # Convertir a matriz numpy\n",
        "    # Asegurarse de que todos los embeddings tengan la misma longitud antes de convertir a numpy\n",
        "    if embeddings:\n",
        "        # Puedes verificar la longitud del primer embedding para la dimensión\n",
        "        # dimension = len(embeddings[0])\n",
        "        # O si usaste SentenceTransformer antes, podrías usar la dimensión de ese modelo (768)\n",
        "        # dimension = 768 # O la dimensión correcta de 'models/embedding-001'\n",
        "\n",
        "        # Convertir a numpy array\n",
        "        embeddings_np_gemini = np.array(embeddings)\n",
        "        print(\"Embeddings de Gemini convertidos a numpy array.\")\n",
        "        print(\"Forma del array de embeddings de Gemini:\", embeddings_np_gemini.shape)\n",
        "\n",
        "        # Nota: Este array de embeddings (embeddings_np_gemini) es diferente al generado por SentenceTransformer.\n",
        "        # Deberás decidir cuál usar para construir tu índice FAISS.\n",
        "        # Si quieres usar estos embeddings de Gemini con el índice FAISS existente,\n",
        "        # necesitarías crear un nuevo índice FAISS con la dimensión correcta para estos embeddings\n",
        "        # y añadir este nuevo array a ese índice.\n",
        "    else:\n",
        "        print(\"No se generaron embeddings.\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generando embeddings para 576 chunks...\n",
            "Embeddings generados para 10/576 chunks.\n",
            "Error al generar embedding para chunk 12: Failed to parse JSON: TypeError: 'NoneType' object is not iterable.. Saltando este chunk.\n",
            "Error al generar embedding para chunk 18: Failed to parse JSON: TypeError: 'NoneType' object is not iterable.. Saltando este chunk.\n",
            "Embeddings generados para 20/576 chunks.\n",
            "Embeddings generados para 30/576 chunks.\n",
            "Embeddings generados para 40/576 chunks.\n",
            "Embeddings generados para 50/576 chunks.\n",
            "Embeddings generados para 60/576 chunks.\n",
            "Embeddings generados para 70/576 chunks.\n",
            "Embeddings generados para 80/576 chunks.\n",
            "Embeddings generados para 90/576 chunks.\n",
            "Embeddings generados para 100/576 chunks.\n",
            "Embeddings generados para 110/576 chunks.\n",
            "Embeddings generados para 120/576 chunks.\n",
            "Embeddings generados para 130/576 chunks.\n",
            "Embeddings generados para 140/576 chunks.\n",
            "Embeddings generados para 150/576 chunks.\n",
            "Embeddings generados para 160/576 chunks.\n",
            "Embeddings generados para 170/576 chunks.\n",
            "Embeddings generados para 180/576 chunks.\n",
            "Embeddings generados para 190/576 chunks.\n",
            "Embeddings generados para 200/576 chunks.\n",
            "Embeddings generados para 210/576 chunks.\n",
            "Embeddings generados para 220/576 chunks.\n",
            "Embeddings generados para 230/576 chunks.\n",
            "Embeddings generados para 240/576 chunks.\n",
            "Embeddings generados para 250/576 chunks.\n",
            "Embeddings generados para 260/576 chunks.\n",
            "Embeddings generados para 270/576 chunks.\n",
            "Embeddings generados para 280/576 chunks.\n",
            "Embeddings generados para 290/576 chunks.\n",
            "Embeddings generados para 300/576 chunks.\n",
            "Embeddings generados para 310/576 chunks.\n",
            "Embeddings generados para 320/576 chunks.\n",
            "Embeddings generados para 330/576 chunks.\n",
            "Embeddings generados para 340/576 chunks.\n",
            "Embeddings generados para 350/576 chunks.\n",
            "Embeddings generados para 360/576 chunks.\n",
            "Embeddings generados para 370/576 chunks.\n",
            "Embeddings generados para 380/576 chunks.\n",
            "Embeddings generados para 390/576 chunks.\n",
            "Embeddings generados para 400/576 chunks.\n",
            "Embeddings generados para 410/576 chunks.\n",
            "Embeddings generados para 420/576 chunks.\n",
            "Embeddings generados para 430/576 chunks.\n",
            "Embeddings generados para 440/576 chunks.\n",
            "Embeddings generados para 450/576 chunks.\n",
            "Embeddings generados para 460/576 chunks.\n",
            "Embeddings generados para 470/576 chunks.\n",
            "Embeddings generados para 480/576 chunks.\n",
            "Embeddings generados para 490/576 chunks.\n",
            "Embeddings generados para 500/576 chunks.\n",
            "Embeddings generados para 510/576 chunks.\n",
            "Embeddings generados para 520/576 chunks.\n",
            "Embeddings generados para 530/576 chunks.\n",
            "Embeddings generados para 540/576 chunks.\n",
            "Embeddings generados para 550/576 chunks.\n",
            "Embeddings generados para 560/576 chunks.\n",
            "Embeddings generados para 570/576 chunks.\n",
            "Generación de embeddings finalizada.\n",
            "Embeddings de Gemini convertidos a numpy array.\n",
            "Forma del array de embeddings de Gemini: (574, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos FAISS (Facebook AI Similarity Search) para almacenar los vectores y poder hacer búsquedas por similitud. Esto nos permite recuperar los chunks más cercanos a una consulta (query) en términos de significado."
      ],
      "metadata": {
        "id": "Gn9rpB1v59TT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos un índice FAISS para búsquedas por similitud\n",
        "# La dimensión debe coincidir con la del array de embeddings de Gemini\n",
        "if 'embeddings_np_gemini' in globals() and embeddings_np_gemini.shape[0] > 0:\n",
        "    dimension = embeddings_np_gemini.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "    # Añadir los vectores de Gemini\n",
        "    index.add(embeddings_np_gemini)\n",
        "    print(f\"Índice FAISS creado con {index.ntotal} vectores de dimensión {dimension}.\")\n",
        "else:\n",
        "    print(\"No se encontraron embeddings de Gemini válidos para crear el índice FAISS.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBEu3vJpxfpt",
        "outputId": "b5fbae0a-85c2-4fdd-ee32-84081ac7b13d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Índice FAISS creado con 574 vectores de dimensión 768.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar los metadatos (puede ser útil para devolver respuestas)\n",
        "# Asegurarse de que la lista de chunks sea coherente con los embeddings generados\n",
        "if 'chunks' in globals() and 'embeddings_np_gemini' in globals() and len(chunks) >= embeddings_np_gemini.shape[0]:\n",
        "    # Si algunos chunks fallaron al generar embeddings, solo guarda los metadatos para los chunks exitosos\n",
        "    # Una forma simple es asumir que los primeros N chunks corresponden a los N embeddings exitosos\n",
        "    # Sin embargo, si hubo saltos (como en la salida anterior), la correspondencia directa puede ser incorrecta.\n",
        "    # Un enfoque más robusto sería mantener un registro de qué chunks generaron embeddings exitosamente.\n",
        "    # Por ahora, asumiremos que el orden se mantuvo para los chunks exitosos y truncamos si es necesario.\n",
        "    chunk_sources = [chunk[\"source\"] for chunk in chunks[:embeddings_np_gemini.shape[0]]]\n",
        "    print(f\"Guardados metadatos para {len(chunk_sources)} chunks correspondientes a los embeddings generados.\")\n",
        "elif 'chunks' in globals():\n",
        "     print(\"Advertencia: El número de chunks no coincide con el número de embeddings generados.\")\n",
        "     chunk_sources = [chunk[\"source\"] for chunk in chunks]\n",
        "else:\n",
        "     print(\"No se encontraron chunks para guardar metadatos.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgyMcJ8cxkn8",
        "outputId": "4f060be9-5610-4fd7-e87c-56da6494f157"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Guardados metadatos para 574 chunks correspondientes a los embeddings generados.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con el índice creado, ahora podemos consultar el sistema en lenguaje natural. FAISS devuelve los chunks más relevantes con base en la similitud semántica entre la consulta y los documentos vectorizados."
      ],
      "metadata": {
        "id": "0sBp-G4C6CJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejemplo de búsqueda\n",
        "query = \"malaria\"\n",
        "# Vectorizar la consulta usando la función get_embeddings\n",
        "query_vec = get_embeddings(query)\n",
        "\n",
        "# Buscar los 5 chunks más similares. index.search espera numpy arrays.\n",
        "# get_embeddings devuelve una lista, la convertimos a numpy array y ajustamos la forma si es necesario\n",
        "query_vec_np = np.array([query_vec])\n",
        "\n",
        "\n",
        "distances, indices = index.search(query_vec_np, k=5)\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(f\"Resultados de la búsqueda para '{query}':\")\n",
        "# Asegurarse de tener chunk_sources disponibles\n",
        "if 'chunk_sources' in globals() and len(chunk_sources) > 0:\n",
        "    for i in indices[0]:\n",
        "        # Asegurarse de que el índice esté dentro del rango de chunks y chunk_sources\n",
        "        if 0 <= i < len(chunks) and 0 <= i < len(chunk_sources):\n",
        "            print(\"🔹 Source:\", chunk_sources[i]) # Usar chunk_sources para el nombre del archivo\n",
        "            # Limitar la salida para mayor legibilidad\n",
        "            print(chunks[i][\"content\"][:500] + \"...\")\n",
        "            print(\"-\" * 80)\n",
        "        else:\n",
        "            print(f\"⚠️ Índice fuera de rango válido para chunks o chunk_sources: {i}\") # Manejar índices inválidos\n",
        "else:\n",
        "    print(\"No se encontraron metadatos de chunks (chunk_sources) para mostrar los resultados.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "CP3OCKuYxmcB",
        "outputId": "404891ac-2fd4-4fd7-d212-dba934a653d7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados de la búsqueda para 'malaria':\n",
            "🔹 Source: 2D6FBE4_3.3.5 - Neglected tropical diseases/170_2D6FBE4_Metadata_2024-01-08.csv\n",
            "a subset of the larger number of people at risk. Mass treatment is limited to those living in districts above a threshold level of prevalence; it does not include all people living in districts with any risk of infection. Individual treatment and care is for those who are or have already been infected; it does not include all contacts and others at risk of infection. This number can better be interpreted as the number of people at a level of risk requiring medical intervention – that is, treatme...\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Source: 442CEA8_3.3.3 - Malaria/170_442CEA8_Dataset_2024-03-08.csv\n",
            "to DIM_TIME.: YEAR | Temporal dimension value (e.g. 2020). Related to DIM_TIME_TYPE.: 2016 | RATE_PER_1000_N: 10.95893 | RATE_PER_1000_NL: 8.33429 | RATE_PER_1000_NU: 13.66553 Indicator Unique ID (seven-digit UID).: 442CEA8 | Full name of Indicator.: Malaria cases | Geographic dimension code type (e.g. \"Country\", \"Region\", \"Global\"). Related to DIM_GEO_CODE_M49.: COUNTRY | Geographic dimension code using M49 SDMX standard (e.g. 004). Related to DIM_GEO_CODE_TYPE.: 170 | Geographic dimension shor...\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Source: 45CA7C8_3.C.1 - Health worker density and distribution/170_45CA7C8_Metadata_2024-05-20.csv\n",
            "190 Geographic entities and groups: Afghanistan, Albania, Algeria, Andorra, Angola, Antigua and Barbuda, Argentina, Armenia, Australia, Austria, Azerbaijan, Bahamas, Bahrain, Bangladesh, Barbados, Belarus, Belgium, Belize, Benin, Bhutan, Bolivia (Plurinational State of), Bosnia and Herzegovina, Botswana, Brazil, Brunei Darussalam, Bulgaria, Burkina Faso, Burundi, Cabo Verde, Cambodia, Cameroon, Canada, Central African Republic, Chad, Chile, China, Colombia, Comoros, Congo, Cook Islands, Costa Ri...\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Source: 1F96863_3.4.1 - cardiovascular disease, cancer, diabetes or chronic respiratory disease/170_1F96863_Metadata_2024-12-18.csv\n",
            "surveys with verbal autopsy, and sample or sentinel registration systems. Expected frequency of data collection: Annual Data providers: National statistics offices and/or ministries of health. Data compilers: World Health Organization (WHO) 🧾 Metadata extraída de 170_1F96863_Metadata_2024-12-18.csv: Name: Probability of premature mortality from NCDs Short name: Probability of dying from any of CVD, cancer, diabetes, CRD between age 30 and exact age 70 (%) Indicator unique identifier: 1F96863 Ind...\n",
            "--------------------------------------------------------------------------------\n",
            "🔹 Source: ED50112_3.9.2 - Unsafe water, unsafe sanitation and lack of hygiene/170_ED50112_Metadata_2024-01-08.csv\n",
            "A06-A09), intestinal nematode infections (ICD-10 code B76-B77, B79), protein-energy malnutrition (ICD-10 code E40-E46) and acute respiratory infections (ICD-10 codes H65-H66, J00-J22, P23, and U04). Direction of progress: Figures decreasing over time demonstrates progress. Framework classifications: Impact Indicator status: Actively collected and reported Most recent data update: 8 January 2024 Publishing institution: WHO Data type: Float Unit of measure: Number of deaths attributable to unsafe ...\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://medium.com/@myscale/how-to-build-a-rag-powered-chatbot-with-google-gemini-and-myscaledb-79c0024cd237"
      ],
      "metadata": {
        "id": "rp8HSJhSwXf9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55122c89"
      },
      "source": [
        "# Configuración del modelo generativo y Generación de respuesta\n",
        "\n",
        "Ahora que podemos recuperar información relevante de nuestra base de datos vectorial, el siguiente paso es utilizar un modelo de lenguaje grande (LLM) para generar una respuesta basada en la consulta del usuario y la información recuperada.\n",
        "\n",
        "Utilizaremos el modelo `gemini-pro` para esta tarea.\n",
        "\n",
        "El proceso general será:\n",
        "\n",
        "1.  El usuario hace una consulta.\n",
        "2.  Vectorizamos la consulta usando la función `get_embeddings`.\n",
        "3.  Buscamos los chunks más relevantes en el índice FAISS usando el vector de la consulta.\n",
        "4.  Tomamos el contenido de los chunks recuperados.\n",
        "5.  Enviamos la consulta original del usuario y el contenido de los chunks recuperados al modelo `gemini-pro` con una instrucción (prompt) para que genere una respuesta informada por esos documentos.\n",
        "6.  Presentamos la respuesta generada al usuario."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "5f96c266",
        "outputId": "4f782d40-0e13-4efb-e255-a1f08a1ad0f9"
      },
      "source": [
        "# Configurar el modelo generativo Gemini\n",
        "\n",
        "model_generative = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "\n",
        "\n",
        "def generate_rag_response(query, retrieved_chunks):\n",
        "    \"\"\"\n",
        "    Genera una respuesta a partir de una consulta y los chunks más relevantes,\n",
        "    usando el modelo generativo Gemini.\n",
        "\n",
        "    Args:\n",
        "        query (str): La pregunta del usuario.\n",
        "        retrieved_chunks (list): Lista de diccionarios, cada uno con la clave 'content'.\n",
        "\n",
        "    Returns:\n",
        "        str: Respuesta generada por Gemini basada en el contexto.\n",
        "    \"\"\"\n",
        "    # Combinar chunks en un único contexto con separación visual\n",
        "    context = \"\\n---\\n\".join([chunk['content'] for chunk in retrieved_chunks])\n",
        "\n",
        "    # Crear prompt para el modelo generativo\n",
        "    prompt = f\"\"\"Usa la siguiente información para responder la pregunta.\n",
        "\n",
        "Contexto:\n",
        "{context}\n",
        "\n",
        "Pregunta:\n",
        "{query}\n",
        "\n",
        "Si no encuentras suficiente información en el contexto, responde que no tienes datos relevantes sobre el tema.\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = model_generative.generate_content(prompt)\n",
        "        return response.text\n",
        "    except Exception as e:\n",
        "        print(f\" Error al generar respuesta con Gemini: {e}\")\n",
        "        return \"Lo siento, no pude generar una respuesta en este momento.\"\n",
        "\n",
        "\n",
        "# ==========\n",
        "# FLUJO DE BÚSQUEDA RAG\n",
        "# ==========\n",
        "\n",
        "query = \"Violencia sexual en Colombia\"\n",
        "\n",
        "if 'get_embeddings' in globals() and 'index' in globals():\n",
        "    try:\n",
        "        # Paso 1: Vectorizar consulta\n",
        "        query_vec = get_embeddings(query)\n",
        "        query_vec_np = np.array([query_vec])\n",
        "\n",
        "        # Paso 2: Buscar los k chunks más relevantes\n",
        "        distances, indices = index.search(query_vec_np, k=5)\n",
        "\n",
        "        # Paso 3: Recuperar contenido de los chunks\n",
        "        retrieved_chunks = [chunks[i] for i in indices[0] if 0 <= i < len(chunks)]\n",
        "\n",
        "        # Paso 4: Generar respuesta\n",
        "        if retrieved_chunks:\n",
        "            print(\"\\n Generando respuesta RAG para tu consulta...\")\n",
        "            rag_response = generate_rag_response(query, retrieved_chunks)\n",
        "            print(\"\\n Respuesta del Chatbot:\\n\")\n",
        "            print(rag_response)\n",
        "        else:\n",
        "            print(\"No se encontraron chunks relevantes.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error durante el proceso de búsqueda RAG: {e}\")\n",
        "else:\n",
        "    print(\" 'get_embeddings' o 'index' no están definidos.\")\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Generando respuesta RAG para tu consulta...\n",
            "\n",
            " Respuesta del Chatbot:\n",
            "\n",
            "No tengo datos relevantes sobre violencia sexual en Colombia en el texto proporcionado.  El texto incluye información sobre enfermedades tropicales desatendidas, homicidios y contaminación del aire, pero no aborda la violencia sexual.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}