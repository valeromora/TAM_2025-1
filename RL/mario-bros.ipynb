{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Super Mario Bros \nEs un entorno visual, la entrada es una imagen de múltiples frames por segundo, lo que se puede usar:\n\n* CNN (Convolutional Neural Networks): Para extraer características espaciales de las imágenes.\n\n* CNN + RNN (LSTM): Si quieres considerar la secuencia temporal (ej. para decisiones dependientes del pasado).\n\n* DQN (Deep Q-Network): Para entornos con acción discreta como Mario.\n\n* Double DQN o Dueling DQN: Para mejorar la estabilidad del entrenamiento.\n\n* Rainbow DQN: Una combinación mejorada de múltiples técnicas DQN.\n\nPara simplificar en TF-Agents, se probara usar DQN + CNN.","metadata":{}},{"cell_type":"markdown","source":"# Set up\nInstalamos las bibliotecas esenciales para ejecutar el entorno `gym-super-mario-bros` con `TF-Agents`. Esto incluye soporte para visualización de videos (`xvfb`, `ffmpeg`), el entorno de Mario Bros, `gym`, y `tf-agents`. También fijamos la versión de `imageio` a 2.4.0 por compatibilidad con la función `imageio.mimsave`, usada más adelante para guardar los videos de las partidas.\n","metadata":{}},{"cell_type":"code","source":"!sudo apt-get -q update -q\n!sudo apt-get install -y -q xvfb ffmpeg freeglut3-dev -q\n!pip install -q 'imageio==2.4.0' -q\n!pip install -q pyvirtualdisplay -q\n!pip install -q tf-agents[reverb] -q\n!pip install -q pyglet -q\n!pip install -q swig -q\n!pip install -q gym[atari,box2d,accept-rom-license] -q #install gym and virtual display \n!pip install -q gym-super-mario-bros -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T16:27:53.007000Z","iopub.execute_input":"2025-07-06T16:27:53.007562Z","iopub.status.idle":"2025-07-06T16:30:10.254367Z","shell.execute_reply.started":"2025-07-06T16:27:53.007534Z","shell.execute_reply":"2025-07-06T16:30:10.253632Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from __future__ import absolute_import, division, print_function\n\nimport base64\nimport imageio\nimport IPython\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport PIL.Image\nimport pyvirtualdisplay\nimport reverb\n\nimport tensorflow as tf\nimport tf_agents\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.environments.wrappers import ActionRepeat\nfrom tf_agents.networks.q_network import QNetwork\nfrom tf_agents.agents.dqn.dqn_agent import DqnAgent\nfrom tf_agents.utils import common\nfrom tf_agents.replay_buffers import TFUniformReplayBuffer\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\nfrom tf_agents.policies import epsilon_greedy_policy\n\nfrom tf_agents.environments import gym_wrapper\nfrom tf_agents.environments import tf_py_environment\n\nimport gym\nfrom nes_py.wrappers import JoypadSpace\n#from gym.wrappers import GrayScaleObservation, ResizeObservation, FrameStack\nimport gym_super_mario_bros\nfrom gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT, RIGHT_ONLY\nimport matplotlib.pyplot as plt\n\n# To get smooth animations\nimport matplotlib.animation as animation\nmatplotlib.rc('animation', html='jshtml')\n\n# Print versions of imported packages\nprint(f\"imageio version: {imageio.__version__}\")\nprint(f\"pyvirtualdisplay version: {pyvirtualdisplay.__version__}\")\n\n# Print TensorFlow version separately\nprint(f\"tensorflow version: {tf.__version__}\")\n\n# Print tf-agents version\ntry:\n    print(f\"tf_agents version: {tf_agents.__version__}\")\nexcept AttributeError:\n    print(\"tf_gents version: not available\")\n\n# Print other versions\nprint(f\"gym version: {gym.__version__}\")\nprint(f\"matplotlib version: {matplotlib.__version__}\")\nprint(f\"PIL version: {PIL.Image.__version__}\")  # Use PIL.Image to get version\n\n# Handle reverb\ntry:\n    import pkg_resources\n    reverb_version = pkg_resources.get_distribution(\"reverb\").version\n    print(f\"reverb version: {reverb_version}\")\nexcept Exception:\n    print(\"reverb version: not available\")\n\n# Handle gym-super-mario-bros\ntry:\n    print(f\"gym-super-mario-bros version: {gym_super_mario_bros.__version__}\")\nexcept AttributeError:\n    print(\"gym-super-mario-bros version: not available\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T16:30:50.811722Z","iopub.execute_input":"2025-07-06T16:30:50.812593Z","iopub.status.idle":"2025-07-06T16:30:50.891423Z","shell.execute_reply.started":"2025-07-06T16:30:50.812569Z","shell.execute_reply":"2025-07-06T16:30:50.890816Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"RL Definitions\n==============\n\n**Environment** The world that an agent interacts with and learns from.\n\n**Action** $a$ : How the Agent responds to the Environment. The set of\nall possible Actions is called *action-space*.\n\n**State** $s$ : The current characteristic of the Environment. The set\nof all possible States the Environment can be in is called\n*state-space*.\n\n**Reward** $r$ : Reward is the key feedback from Environment to Agent.\nIt is what drives the Agent to learn and to change its future action. An\naggregation of rewards over multiple time steps is called **Return**.\n\n**Optimal Action-Value function** $Q^*(s,a)$ : Gives the expected return\nif you start in state $s$, take an arbitrary action $a$, and then for\neach future time step take the action that maximizes returns. $Q$ can be\nsaid to stand for the \"quality\" of the action in a state. We try to\napproximate this function.\n","metadata":{}},{"cell_type":"markdown","source":"Environment\n===========\n\nInitialize Environment\n----------------------\n\nUsamos `gym_super_mario_bros` para crear el entorno de juego `SuperMarioBros-v2`. Luego lo envolvemos con `JoypadSpace` para simplificar el espacio de acciones. En este caso, usamos `COMPLEX_MOVEMENT`, que ofrece combinaciones de movimientos más ricas como correr y saltar a la vez. También imprimimos las distintas configuraciones de movimiento disponibles para explorar otras opciones más simples como `RIGHT_ONLY` o `SIMPLE_MOVEMENT`.\n\nFinalmente, hacemos un `reset` y ejecutamos un paso aleatorio para renderizar una imagen del entorno como verificación visual.\n\n","metadata":{}},{"cell_type":"code","source":"env = gym_super_mario_bros.make('SuperMarioBros-v2')\nenv = JoypadSpace(env, COMPLEX_MOVEMENT)\nprint(SIMPLE_MOVEMENT)\nprint(COMPLEX_MOVEMENT)\nprint(RIGHT_ONLY)\nstate = env.reset()\nenv.step(env._action_space.sample())\nimg = env.render(mode=\"rgb_array\")\nplt.figure(figsize=(4, 6))\nplt.imshow(img)\nplt.axis(\"off\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T16:31:01.628677Z","iopub.execute_input":"2025-07-06T16:31:01.628923Z","iopub.status.idle":"2025-07-06T16:31:02.119712Z","shell.execute_reply.started":"2025-07-06T16:31:01.628908Z","shell.execute_reply":"2025-07-06T16:31:02.119124Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Preprocess Environment\n======================\n\nDefinimos una función `create_environment()` que envuelve el entorno de `SuperMarioBros-v2` con varias transformaciones útiles para facilitar el entrenamiento del agente. Estas transformaciones incluyen:\n\n- `JoypadSpace`: se usa `SIMPLE_MOVEMENT` para reducir la complejidad del espacio de acción.\n- `GrayScaleObservation`: convierte las imágenes a escala de grises para reducir la dimensionalidad.\n- `ResizeObservation`: redimensiona las imágenes a 84x84 píxeles, un estándar común en entornos de juegos.\n- `FrameStack`: apila las últimas 4 observaciones, permitiendo que el agente tenga noción de movimiento.\n- `GymWrapper`: adapta el entorno para que sea compatible con TF-Agents.","metadata":{}},{"cell_type":"code","source":"def create_mario_environment(\n    level='SuperMarioBros-v2',\n    movement=SIMPLE_MOVEMENT,\n    grayscale=True,\n    resize=84,\n    stack_frames=4\n):\n    env = gym_super_mario_bros.make(level)\n    env = JoypadSpace(env, movement)\n\n    if grayscale:\n        env = gym.wrappers.GrayScaleObservation(env, keep_dim=True)\n    if resize:\n        env = gym.wrappers.ResizeObservation(env, resize)\n    if stack_frames:\n        env = gym.wrappers.FrameStack(env, stack_frames)\n\n    env = gym_wrapper.GymWrapper(env)\n    return env","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T16:31:13.418249Z","iopub.execute_input":"2025-07-06T16:31:13.418546Z","iopub.status.idle":"2025-07-06T16:31:13.423764Z","shell.execute_reply.started":"2025-07-06T16:31:13.418525Z","shell.execute_reply":"2025-07-06T16:31:13.422984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Entrenamiento y evaluación con entorno más simple\ntrain_py_env = create_mario_environment(level='SuperMarioBros-v2', movement=SIMPLE_MOVEMENT)\neval_py_env = create_mario_environment(level='SuperMarioBros-v2', movement=SIMPLE_MOVEMENT)\n\n# Entrenamiento y evaluación con entorno más complejo (nivel específico y más acciones)\ntrain_py_env1 = create_mario_environment(level='SuperMarioBros-1-1-v2', movement=COMPLEX_MOVEMENT)\neval_py_env1 = create_mario_environment(level='SuperMarioBros-1-1-v2', movement=COMPLEX_MOVEMENT)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T16:31:16.199770Z","iopub.execute_input":"2025-07-06T16:31:16.200068Z","iopub.status.idle":"2025-07-06T16:31:17.779887Z","shell.execute_reply.started":"2025-07-06T16:31:16.200045Z","shell.execute_reply":"2025-07-06T16:31:17.779354Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Conversión a entornos compatibles con TF-Agents\n\nTF-Agents requiere que los entornos estén en formato `TFPyEnvironment` para interactuar correctamente con los agentes y las políticas. Por eso, convertimos los entornos de entrenamiento y evaluación previamente definidos.\n\nLos entornos convertidos (`train_env`, `eval_env`) se usarán durante el proceso de entrenamiento, evaluación y generación de episodios.\n","metadata":{}},{"cell_type":"code","source":"train_env = tf_py_environment.TFPyEnvironment(\n    create_mario_environment(level='SuperMarioBros-v2', movement=SIMPLE_MOVEMENT)\n)\n\neval_env = tf_py_environment.TFPyEnvironment(\n    create_mario_environment(level='SuperMarioBros-v2', movement=SIMPLE_MOVEMENT)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T16:33:38.986875Z","iopub.execute_input":"2025-07-06T16:33:38.987470Z","iopub.status.idle":"2025-07-06T16:33:39.739944Z","shell.execute_reply.started":"2025-07-06T16:33:38.987448Z","shell.execute_reply":"2025-07-06T16:33:39.739086Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Definición de la red Q y del agente DQN\n\nCreamos una red Q convolucional (`QNetwork`) para que el agente pueda procesar imágenes del entorno y tomar decisiones. Las observaciones (imágenes) se normalizan a valores entre 0 y 1 para estabilizar el entrenamiento. La arquitectura está compuesta por tres capas convolucionales y una capa totalmente conectada con 512 unidades.\n\nUtilizamos un optimizador `RMSProp`, recomendado para entornos de juegos con imágenes, y configuramos una política epsilon-greedy con `epsilon` que decae linealmente desde 1.0 hasta 0.01 en 10,000 pasos, para favorecer la exploración al inicio y la explotación conforme avanza el entrenamiento.\n\nFinalmente, instanciamos el agente `DqnAgent`, con una función de pérdida basada en errores cuadráticos y una tasa de descuento (`gamma`) de 0.99.\n\n| Capa        | Filtros | Tamaño kernel | Stride |\n|-------------|---------|----------------|--------|\n| Conv2D #1   | 32      | (8, 8)         | 4      |\n| Conv2D #2   | 64      | (4, 4)         | 2      |\n| Conv2D #3   | 64      | (3, 3)         | 1      |\n| Dense       | 512     | -              | -      |\n","metadata":{}},{"cell_type":"code","source":"# Definir la red Q\npreprocessing_layer = tf.keras.layers.Lambda(lambda x: tf.cast(x, np.float32) / 255.)  # Se crea una capa de preprocesamiento para escalar las imágenes entre 0 y 1\nconv_layer_params = [  # Se definen los parámetros para las capas convolucionales\n    (32, (8, 8), 4),  # Se configura la primera capa convolucional (número de filtros, tamaño del kernel, paso)\n    (64, (4, 4), 2),  # Se configura la segunda capa convolucional\n    (64, (3, 3), 1),  # Se configura la tercera capa convolucional\n]\n\nfc_layer_params = [512]  # Se definen los parámetros para las capas totalmente conectadas 'neuronas'\n\n# Crear la red Q\nq_net = QNetwork(\n    input_tensor_spec = train_env.observation_spec(),  # Se especifica la entrada del entorno de entrenamiento\n    action_spec = train_env.action_spec(),  # Se especifican las acciones del entorno de entrenamiento\n    preprocessing_layers = preprocessing_layer,  # Se asigna la capa de preprocesamiento\n    conv_layer_params = conv_layer_params,  # Se asignan los parámetros de las capas convolucionales\n    fc_layer_params = fc_layer_params  # Se asignan los parámetros de las capas totalmente conectadas\n)\n\n\n# Definir el optimizador\noptimizer = tf.compat.v1.train.RMSPropOptimizer(  # Se define el optimizador RMSProp\n    learning_rate = 2.5e-4,  # Se establece la tasa de aprendizaje\n    decay = 0.95,  # Se configura el decaimiento del optimizador\n    momentum = 0.0,  # Se establece el momentum del optimizador\n    epsilon = 0.01,  # Se configura el epsilon para el optimizador\n    centered = True  # Se utiliza la versión centrada del RMSProp\n)\n# Definir el contador global de pasos\ntrain_step_counter = tf.Variable(0)\n\nepsilon = tf.compat.v1.train.polynomial_decay(\n    learning_rate=1.0,  # Valor inicial de epsilon\n    global_step=train_step_counter,\n    decay_steps=10000,  # Número de pasos para reducir epsilon\n    end_learning_rate=0.01,  # Valor mínimo de epsilon\n    power=1.0)  # Controla la tasa de decay (1.0 es lineal)\n\n# Usar el decay en el agente\n# Inicializar el agente DQN\nagent = DqnAgent(\n    time_step_spec = train_env.time_step_spec(),  # Se especifica el tiempo del entorno de entrenamiento\n    action_spec = train_env.action_spec(),  # Se especifican las acciones del entorno de entrenamiento\n    q_network = q_net,  # Se asigna la red Q que se utilizará\n    optimizer = optimizer,  # Se asigna el optimizador para el agente\n    td_errors_loss_fn = common.element_wise_squared_loss,  # Se define la función de pérdida para errores temporales\n    train_step_counter = train_step_counter,  # Se asigna el contador de pasos de entrenamiento\n    gamma = 0.99,  # Se establece el factor de descuento\n    epsilon_greedy = epsilon,  # Se configura la probabilidad de elegir una acción aleatoria\n    target_update_period = 10000  # Se establece la frecuencia para actualizar el objetivo\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T16:36:19.716010Z","iopub.execute_input":"2025-07-06T16:36:19.716271Z","iopub.status.idle":"2025-07-06T16:36:21.052093Z","shell.execute_reply.started":"2025-07-06T16:36:19.716255Z","shell.execute_reply":"2025-07-06T16:36:21.051582Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inicialización del agente y configuración del replay buffer\n\nInicializamos el agente `DqnAgent` con `agent.initialize()` para preparar todas sus variables internas.\n\nLuego, creamos un `TFUniformReplayBuffer`, que se encargará de almacenar las transiciones observadas por el agente. Este buffer es esencial en el algoritmo DQN, ya que permite muestrear experiencias de manera aleatoria durante el entrenamiento, lo cual rompe la correlación temporal entre datos consecutivos y estabiliza el aprendizaje.\n\n- `data_spec`: define la estructura de los datos que se almacenarán (observaciones, acciones, recompensas, etc.).\n- `batch_size`: viene del entorno (`train_env`) y define cuántos entornos paralelos se están usando (normalmente 1 si no se usa vectorización).\n- `max_length`: controla la cantidad máxima de transiciones que el buffer puede almacenar (aquí 100,000).\n\n","metadata":{}},{"cell_type":"code","source":"agent._time_step_spec","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T16:36:25.431545Z","iopub.execute_input":"2025-07-06T16:36:25.431816Z","iopub.status.idle":"2025-07-06T16:36:25.437812Z","shell.execute_reply.started":"2025-07-06T16:36:25.431797Z","shell.execute_reply":"2025-07-06T16:36:25.437104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"agent.initialize()  # Se inicializa el agente DQN\n\n# Create the replay buffer\nreplay_buffer = TFUniformReplayBuffer(\n    data_spec = agent.collect_data_spec,  # Se especifica la estructura de los datos que se recogerán\n    batch_size = train_env.batch_size,  # Se establece el tamaño del lote para el buffer, acciones, recompensas y otros elementos relevantes.\n    max_length = 100000  # Se define la longitud máxima del buffer de repetición, se actualiza\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T16:36:27.346899Z","iopub.execute_input":"2025-07-06T16:36:27.347209Z","iopub.status.idle":"2025-07-06T16:36:28.189396Z","shell.execute_reply.started":"2025-07-06T16:36:27.347183Z","shell.execute_reply":"2025-07-06T16:36:28.188824Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Train env batch size:\", train_env.batch_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T16:37:03.882758Z","iopub.execute_input":"2025-07-06T16:37:03.883015Z","iopub.status.idle":"2025-07-06T16:37:03.887544Z","shell.execute_reply.started":"2025-07-06T16:37:03.883000Z","shell.execute_reply":"2025-07-06T16:37:03.886740Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Recolección de experiencia inicial y preparación del dataset\n\nAntes de entrenar al agente, necesitamos llenar el replay buffer con experiencias. Esto se hace utilizando una política aleatoria (`RandomTFPolicy`), lo cual permite explorar el entorno de forma uniforme sin sesgo. Aquí recolectamos 10,000 pasos.\n\nLuego, convertimos el buffer en un conjunto de datos (`as_dataset`), que puede usarse durante el entrenamiento para muestrear secuencias de transiciones. Este dataset se preprocesa usando `prefetch()` para mejorar la eficiencia y evitar cuellos de botella en la carga de datos.\n","metadata":{}},{"cell_type":"code","source":"# Función para recolectar experiencia\ndef collect_step(environment, policy, buffer):\n    time_step = environment.current_time_step()  # Se obtiene el estado actual del entorno\n    action_step = policy.action(time_step)  # Se calcula la acción a partir del estado actual\n    next_time_step = environment.step(action_step.action)  # Se aplica la acción en el entorno y se obtiene el siguiente estado\n    traj = trajectory.from_transition(time_step, action_step, next_time_step)  # Se crea una trayectoria a partir de la transición\n    buffer.add_batch(traj)  # Se añade la trayectoria al buffer de experiencias\n\n# Se recolectan datos iniciales con una política aleatoria\nrandom_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())  # Se define una política aleatoria\ninitial_collect_steps = 10000  # Se establece el número de pasos iniciales de recolección\nfor _ in range(initial_collect_steps):\n    collect_step(train_env, random_policy, replay_buffer)  # Se recolecta experiencia utilizando la política aleatoria\n\n# Se prepara el conjunto de datos\ndataset = replay_buffer.as_dataset(  # Se convierte el buffer en un conjunto de datos\n    num_parallel_calls=3,  # Se establece el número de llamadas paralelas\n    sample_batch_size=64,  # Se define el tamaño del lote de muestra\n    num_steps=2  # Se especifica el número de pasos a considerar en cada muestra\n).prefetch(3)  # Se pre-carga el conjunto de datos\niterator = iter(dataset)  # Se crea un iterador para el conjunto de datos","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T16:40:10.912075Z","iopub.execute_input":"2025-07-06T16:40:10.912404Z","iopub.status.idle":"2025-07-06T16:41:09.036719Z","shell.execute_reply.started":"2025-07-06T16:40:10.912385Z","shell.execute_reply":"2025-07-06T16:41:09.035679Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Función para evaluar el desempeño del agente\n\nLa función `evaluate_agent` ejecuta el agente sobre el entorno de evaluación por un número dado de episodios (`num_episodes`) y devuelve la recompensa promedio obtenida. Durante cada episodio, el agente selecciona acciones usando su política actual hasta que el episodio termina. Se acumulan las recompensas recibidas en cada paso y finalmente se calcula el promedio.\n\nEsta métrica nos permite medir el progreso y desempeño real del agente sin exploración (política explotativa).","metadata":{}},{"cell_type":"code","source":"def evaluate_agent(agent, eval_env, num_episodes=1):\n    total_reward = 0.0\n    for episode in range(num_episodes):\n        time_step = eval_env.reset()\n        policy_state = agent.policy.get_initial_state(eval_env.batch_size)\n        episode_reward = 0\n        \n        while not time_step.is_last():\n            action_step = agent.policy.action(time_step, policy_state)\n            time_step = eval_env.step(action_step.action)\n            episode_reward += time_step.reward.numpy() #\n            \n        total_reward += episode_reward\n    avg_reward = total_reward / num_episodes\n    return avg_reward","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Entrenamiento del agente DQN\n\nEjecutamos un loop de entrenamiento que realiza lo siguiente en cada iteración:\n\n- Recolecta nuevas experiencias utilizando la política actual del agente (`agent.collect_policy`).\n- Extrae un batch de experiencias del replay buffer y realiza una actualización del agente usando estas muestras.\n- Cada cierto número de iteraciones (`log_interval`), imprime la pérdida para monitorear el progreso.\n\nSe define un directorio de checkpoint para guardar el modelo periódicamente (comentado por ahora).\n\nSe planea evaluar el agente cada `eval_interval` iteraciones, aunque esta parte está comentada y se puede habilitar para monitorear rendimiento.\n","metadata":{}},{"cell_type":"code","source":"# Entrenamiento del agente\nnum_iterations = 145450  # Se ajusta este valor según los límites computacionales de Kaggle 350000 -> 145500\ncollect_steps_per_iteration = 1  # Se define el número de pasos de recolección por iteración\nlog_interval = 500  # Se establece el intervalo para los registros\neval_interval = 30000  # Evaluar el agente cada 1000 iteraciones\n\ntrain_losses = []\neval_rewards = []\neval_iterations = []  # Lista para guardar las iteraciones en las que evalúas\n\ncheckpoint_dir = 'checkpoints/'\ncheckpoint = tf.train.Checkpoint(agent=agent)\n\n# Guardar el modelo cada X iteraciones\nsave_interval = 40000  # Guardar cada 5000 iteraciones\n\nfor iteration in range(num_iterations):\n    # Recolectar experiencia\n    for _ in range(collect_steps_per_iteration):\n        collect_step(train_env, agent.collect_policy, replay_buffer)  # Se llama a la función para recolectar experiencias\n\n    # Muestra una experiencia del buffer y entrena al agente\n    experience, _ = next(iterator)  # Se extrae un lote de experiencia del buffer\n    train_loss = agent.train(experience).loss  # Se entrena al agente y se obtiene la pérdida\n    \n    if iteration % log_interval == 0:  # para cada intervalo\n        print(f'Iteración: {iteration}, Pérdida: {train_loss}')  # Se imprime la iteración y la pérdida\n        train_losses.append(train_loss.numpy())\n        \n    if iteration % eval_interval == 0:\n        avg_reward = evaluate_agent(agent, eval_env, num_episodes=5)  # Mejor hacer varios episodios\n        print(f'Evaluación en iteración {iteration}: Recompensa media = {avg_reward}')\n        eval_rewards.append(avg_reward)\n        eval_iterations.append(iteration)  # Guardar iteración actual\n\n    if iteration % save_interval == 0:\n        checkpoint.save(file_prefix=checkpoint_dir)\n        print(f'Modelo guardado en la iteración {iteration}')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-06T16:59:30.271912Z","iopub.execute_input":"2025-07-06T16:59:30.272170Z","iopub.status.idle":"2025-07-06T16:59:30.331739Z","shell.execute_reply.started":"2025-07-06T16:59:30.272135Z","shell.execute_reply":"2025-07-06T16:59:30.330829Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualización del agente en acción\n\nConfiguramos una pantalla virtual para renderizar el entorno sin necesidad de una ventana gráfica visible, lo cual es útil en entornos remotos como Kaggle.\n\nDefinimos funciones para:\n\n- Capturar los frames del entorno mientras el agente ejecuta su política (`run_and_visualize`).\n- Crear una animación a partir de esos frames (`plot_animation`).\n\nFinalmente, mostramos la animación directamente en el notebook para observar el comportamiento del agente entrenado.\n","metadata":{}},{"cell_type":"code","source":"# Configurar una pantalla virtual para renderizar entornos de OpenAI Gym\ndisplay = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()  # Se inicia la pantalla virtual\n\n# Funciones de visualización proporcionadas\ndef update_scene(num, frames, patch):  # Se define la función para actualizar la escena\n    patch.set_data(frames[num])  # Se actualizan los datos del frame actual\n    return patch  # Se devuelve el parche actualizado\n\ndef plot_animation(frames, repeat=False, interval=40):  # Se define la función para crear la animación\n    fig = plt.figure()  # Se crea una nueva figura\n    patch = plt.imshow(frames[0])  # Se muestra el primer frame\n    plt.axis('off')  # Se ocultan los ejes\n    anim = animation.FuncAnimation(  # Se crea la animación\n        fig, update_scene, fargs=(frames, patch),\n        frames=len(frames), repeat=repeat, interval=interval)\n    plt.close()  # Se cierra la figura para no mostrarla dos veces\n    return anim  # Se devuelve la animación creada\n\n# Recoger frames para la visualización\ndef run_and_visualize(agent, env):\n    frames = []\n    time_step = env.reset()\n    policy_state = agent.policy.get_initial_state(env.batch_size)\n    while not time_step.is_last():\n        action_step = agent.policy.action(time_step, policy_state)\n        policy_state = action_step.state\n        time_step = env.step(action_step.action)\n        frame = np.squeeze(env.render())\n         if frame.shape[-1] == 1:\n            frame = np.repeat(frame, 3, axis=-1)\n        frames.append(np.squeeze(frame))\n    print(f\"Total frames collected: {len(frames)}\")\n    return frames\n\n# Ejecutar el agente y recoger marcos\nframes1 = run_and_visualize(agent, eval_env)  # Se ejecuta la función para recoger marcos\n\n# Crear y mostrar la animación\nanim1 = plot_animation(frames1)  # Se crea la animación a partir de los marcos\n\n# Mostrar la animación en Jupyter Notebook\nfrom IPython.display import HTML  # Se importa la librería necesaria\nHTML(anim1.to_jshtml())  # Se convierte la animación a formato HTML y se muestra","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualización de la política aleatoria\n\nDefinimos una política simple que elige acciones al azar entre las posibles del entorno.\n\nImplementamos la función `run_and_visualize1` que ejecuta esta política aleatoria durante un número fijo de pasos (8000) en el entorno de evaluación, capturando los frames para luego generar una animación.\n\nEsto nos permite comparar visualmente el comportamiento del agente entrenado versus un agente que actúa sin aprendizaje.\n","metadata":{}},{"cell_type":"code","source":"train_env1 = tf_py_environment.TFPyEnvironment(train_py_env1)\neval_env1 = tf_py_environment.TFPyEnvironment(eval_py_env1)\n\n# Configurar una pantalla virtual para renderizar entornos de OpenAI Gym\ndisplay = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()  # Se inicia la pantalla virtual\n\n# Funciones de visualización proporcionadas\ndef update_scene(num, frames, patch):  # Se define la función para actualizar la escena\n    patch.set_data(frames[num])  # Se actualizan los datos del frame actual\n    return patch  # Se devuelve el parche actualizado\n\ndef plot_animation1(frames, repeat=False, interval=40):  # Se define la función para crear la animación\n    fig = plt.figure()  # Se crea una nueva figura\n    patch = plt.imshow(frames[0])  # Se muestra el primer frame\n    plt.axis('off')  # Se ocultan los ejes\n    anim = animation.FuncAnimation(  # Se crea la animación\n        fig, update_scene, fargs=(frames, patch),\n        frames=len(frames), repeat=repeat, interval=interval)\n    plt.close()  # Se cierra la figura para no mostrarla dos veces\n    return anim  # Se devuelve la animación creada\ndef random_policy():\n    return np.random.choice(len(SIMPLE_MOVEMENT))\n# Recoger frames para la visualización\ndef run_and_visualize1(env):\n    frames = []\n    time_step = env.reset()\n    \n    for i in range(8000):\n        action_step = random_policy()\n        time_step = env.step(action_step)\n        \n        # Obtener el frame y asegurarse de que es un array numpy\n        frame = np.squeeze(env.render())\n               \n        frames.append(frame)\n        i += 1\n\n    print(f\"Total frames collected: {len(frames)}\")\n    return frames\n\n\n# Ejecutar el agente y recoger marcos\nframes_random = run_and_visualize1(eval_env1)  # Se ejecuta la función para recoger marcos\n\n# Crear y mostrar la animación\nanim = plot_animation1(frames_random)  # Se crea la animación a partir de los marcos\n\n# Mostrar la animación en Jupyter Notebook\nfrom IPython.display import HTML  # Se importa la librería necesaria\nHTML(anim.to_jshtml())  # Se convierte la animación a formato HTML y se muestra","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Gráfica de recompensa promedio\n\nEsta gráfica muestra cómo la recompensa promedio obtenida por el agente en el entorno de evaluación evoluciona a lo largo de las iteraciones de entrenamiento. Un aumento en esta métrica indica que el agente está aprendiendo a tomar mejores decisiones para maximizar la recompensa acumulada.\n","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.plot(eval_iterations, eval_rewards, marker='o', linestyle='-')\nplt.title('Evolución de la recompensa promedio durante el entrenamiento')\nplt.xlabel('Iteraciones')\nplt.ylabel('Recompensa promedio')\nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}