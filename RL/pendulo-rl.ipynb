{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":528.897805,"end_time":"2025-06-19T00:41:35.418844","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-06-19T00:32:46.521039","version":"2.4.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction\n","metadata":{"id":"lsaQlK8fFQqH","papermill":{"duration":0.008728,"end_time":"2025-06-19T00:32:49.588954","exception":false,"start_time":"2025-06-19T00:32:49.580226","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Setup","metadata":{"id":"1u9QVVsShC9X","papermill":{"duration":0.008561,"end_time":"2025-06-19T00:32:49.624158","exception":false,"start_time":"2025-06-19T00:32:49.615597","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!sudo apt-get update -y\n!sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n#!pip install imageio\n!pip install \"imageio>=2.33,<2.35\"\n!pip install pyvirtualdisplay\n!pip install pyglet\n!pip install tf-agents[reverb]\n!pip install gymnasium","metadata":{"execution":{"iopub.status.busy":"2025-06-28T03:10:04.436022Z","iopub.execute_input":"2025-06-28T03:10:04.436754Z","iopub.status.idle":"2025-06-28T03:10:26.130657Z","shell.execute_reply.started":"2025-06-28T03:10:04.436718Z","shell.execute_reply":"2025-06-28T03:10:26.128809Z"},"id":"KEHR2Ui-lo8O","papermill":{"duration":136.235805,"end_time":"2025-06-19T00:35:05.886743","exception":false,"start_time":"2025-06-19T00:32:49.650938","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from __future__ import absolute_import, division, print_function\n\nimport base64\nimport imageio\nimport IPython\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport PIL.Image\nimport pyvirtualdisplay\nimport reverb\nimport os\n\nimport tensorflow as tf\n\nfrom tf_agents.agents.ddpg import ddpg_agent\nfrom tf_agents.drivers import py_driver\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import tf_metrics\n#from tf_agents.networks import ActorNetwork, CriticNetwork\nfrom tf_agents.networks import network\nfrom tf_agents.networks.actor_distribution_network import ActorDistributionNetwork\nfrom tf_agents.policies import py_tf_eager_policy\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.replay_buffers import reverb_replay_buffer\nfrom tf_agents.replay_buffers import reverb_utils\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.utils import common\nimport gymnasium as gym \nfrom tf_agents.trajectories import time_step as ts","metadata":{"id":"sMitx5qSgJk1","papermill":{"duration":5.998194,"end_time":"2025-06-19T00:35:11.911387","exception":false,"start_time":"2025-06-19T00:35:05.913193","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:10:26.132666Z","iopub.execute_input":"2025-06-28T03:10:26.132914Z","iopub.status.idle":"2025-06-28T03:10:26.189211Z","shell.execute_reply.started":"2025-06-28T03:10:26.132892Z","shell.execute_reply":"2025-06-28T03:10:26.188653Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set up a virtual display for rendering OpenAI gym environments.\ndisplay = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()","metadata":{"id":"J6HsdS5GbSjd","papermill":{"duration":0.466138,"end_time":"2025-06-19T00:35:12.403688","exception":false,"start_time":"2025-06-19T00:35:11.937550","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:10:26.189889Z","iopub.execute_input":"2025-06-28T03:10:26.190142Z","iopub.status.idle":"2025-06-28T03:10:26.277399Z","shell.execute_reply.started":"2025-06-28T03:10:26.190123Z","shell.execute_reply":"2025-06-28T03:10:26.276801Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tf.version.VERSION","metadata":{"id":"NspmzG4nP3b9","papermill":{"duration":0.034545,"end_time":"2025-06-19T00:35:12.465054","exception":false,"start_time":"2025-06-19T00:35:12.430509","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:10:26.279084Z","iopub.execute_input":"2025-06-28T03:10:26.279321Z","iopub.status.idle":"2025-06-28T03:10:26.285164Z","shell.execute_reply.started":"2025-06-28T03:10:26.279279Z","shell.execute_reply":"2025-06-28T03:10:26.284608Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Hyperparameters\n| Parámetro                           | Comentario                                                                              |\n| ----------------------------------- | --------------------------------------------------------------------------------------- |\n| `num_iterations = 20000`            | 20k pasos es razonable para ver progreso. Se puede aumentar.\t\t\t        |\n| `initial_collect_steps = 1000`      | Está bien para llenar el buffer inicialmente.                                           |\n| `collect_steps_per_iteration = 1`   | Está bien para DDPG, que se entrena cada paso.                                          |\n| `replay_buffer_max_length = 100000` | Más que suficiente.\t\t\t\t                                        |\n| `batch_size = 64`                   | Estándar y funciona bien. Se puede probar 128.    \t                                |\n| `learning_rate = 1e-3`              | Común para DDPG. Si el entrenamiento es inestable, prueba 1e-4.                         |\n| `log_interval = 200`                | Bueno para monitorear progreso.                                                         |\n| `num_eval_episodes = 10`            | Correcto para promediar bien el desempeño.                                              |\n| `eval_interval = 500`               | Evaluar cada 500 pasos es razonable. Puedes hacerlo cada 1000 si quieres menos ruido.   |\n","metadata":{"id":"LmC0NDhdLIKY","papermill":{"duration":0.025214,"end_time":"2025-06-19T00:35:12.516243","exception":false,"start_time":"2025-06-19T00:35:12.491029","status":"completed"},"tags":[]}},{"cell_type":"code","source":"num_iterations = 20000 # @param {type:\"integer\"}\n\ninitial_collect_steps = 1000  # @param {type:\"integer\"}\ncollect_steps_per_iteration =   1# @param {type:\"integer\"}\nreplay_buffer_max_length = 100000  # @param {type:\"integer\"}\n\nbatch_size = 64  # @param {type:\"integer\"}\nlearning_rate = 1e-3  # @param {type:\"number\"}\nlog_interval = 200  # @param {type:\"integer\"}\n\nnum_eval_episodes = 10  # @param {type:\"integer\"}\neval_interval = 500  # @param {type:\"integer\"}","metadata":{"id":"HC1kNrOsLSIZ","papermill":{"duration":0.032389,"end_time":"2025-06-19T00:35:12.574021","exception":false,"start_time":"2025-06-19T00:35:12.541632","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:10:26.285822Z","iopub.execute_input":"2025-06-28T03:10:26.286040Z","iopub.status.idle":"2025-06-28T03:10:26.299910Z","shell.execute_reply.started":"2025-06-28T03:10:26.286026Z","shell.execute_reply":"2025-06-28T03:10:26.299120Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Environment\n\nIn Reinforcement Learning (RL), an environment represents the task or problem to be solved. Standard environments can be created in TF-Agents using `tf_agents.environments` suites. TF-Agents has suites for loading environments from sources such as the OpenAI Gym, Atari, and DM Control.\n\nLoad the Pendulum environment from the OpenAI Gym suite.","metadata":{"id":"VMsJC3DEgI0x","papermill":{"duration":0.024891,"end_time":"2025-06-19T00:35:12.624410","exception":false,"start_time":"2025-06-19T00:35:12.599519","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Nombre del entorno\nenv_name = 'Pendulum-v1'\n\n# Cargar entorno de entrenamiento y evaluación\ntrain_py_env = suite_gym.load(env_name)\neval_py_env = suite_gym.load(env_name)\n\n# Convertir a entornos de TensorFlow\ntrain_env = tf_py_environment.TFPyEnvironment(train_py_env)\neval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n\n# Verificar especificaciones\nprint(\"Observación:\", train_env.observation_spec())\nprint(\"Acción:\", train_env.action_spec())","metadata":{"id":"pYEz-S9gEv2-","papermill":{"duration":0.070379,"end_time":"2025-06-19T00:35:12.720366","exception":false,"start_time":"2025-06-19T00:35:12.649987","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:10:26.300798Z","iopub.execute_input":"2025-06-28T03:10:26.301084Z","iopub.status.idle":"2025-06-28T03:10:26.364434Z","shell.execute_reply.started":"2025-06-28T03:10:26.301056Z","shell.execute_reply":"2025-06-28T03:10:26.363768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cargar entorno Gymnasium con render_mode adecuado\nrender_env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n\n# Reiniciar entorno\nrender_env.reset()\n\n# Renderizar una imagen y mostrarla\nimg = render_env.render()\nPIL.Image.fromarray(img)","metadata":{"id":"RlO7WIQHu_7D","papermill":{"duration":0.526192,"end_time":"2025-06-19T00:35:13.323175","exception":false,"start_time":"2025-06-19T00:35:12.796983","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T04:09:54.313090Z","iopub.execute_input":"2025-06-28T04:09:54.313673Z","iopub.status.idle":"2025-06-28T04:09:54.336048Z","shell.execute_reply.started":"2025-06-28T04:09:54.313651Z","shell.execute_reply":"2025-06-28T04:09:54.335089Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The `environment.step` method takes an `action` in the environment and returns a `TimeStep` tuple containing the next observation of the environment and the reward for the action.\n\nThe `time_step_spec()` method returns the specification for the `TimeStep` tuple. Its `observation` attribute shows the shape of observations, the data types, and the ranges of allowed values. The `reward` attribute shows the same details for the reward.\n","metadata":{"id":"B9_lskPOey18","papermill":{"duration":0.02602,"end_time":"2025-06-19T00:35:13.377939","exception":false,"start_time":"2025-06-19T00:35:13.351919","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print('Observation Spec:')\nprint(train_env.observation_spec())","metadata":{"id":"exDv57iHfwQV","papermill":{"duration":0.036436,"end_time":"2025-06-19T00:35:13.440269","exception":false,"start_time":"2025-06-19T00:35:13.403833","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:10:26.458738Z","iopub.execute_input":"2025-06-28T03:10:26.459025Z","iopub.status.idle":"2025-06-28T03:10:26.464594Z","shell.execute_reply.started":"2025-06-28T03:10:26.458999Z","shell.execute_reply":"2025-06-28T03:10:26.463768Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This describes the observation space — the input to your RL agent.\n\n* Shape: (3,) → the observation is a 3D vector.\n\n* Meaning of components:\n    1. cos(θ) → pendulum angle in cosine form.\n    2. sin(θ) → same angle in sine form.\n    3. θ_dot → angular velocity of the pendulum.\n\n* Values are bounded:\n    1. cos/sin are between -1 and 1\n    2. angular velocity is between -8 and 8","metadata":{}},{"cell_type":"code","source":"print('Reward Spec:')\nprint(train_env.reward_spec())","metadata":{"id":"UxiSyCbBUQPi","papermill":{"duration":0.033918,"end_time":"2025-06-19T00:35:13.500371","exception":false,"start_time":"2025-06-19T00:35:13.466453","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:10:26.465374Z","iopub.execute_input":"2025-06-28T03:10:26.465633Z","iopub.status.idle":"2025-06-28T03:10:26.485339Z","shell.execute_reply.started":"2025-06-28T03:10:26.465606Z","shell.execute_reply":"2025-06-28T03:10:26.484432Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This means each time step will return a scalar reward (float32). No bounds are specified, but you know from Pendulum-v1’s definition that:\n\n* Reward is typically negative.\n\n* Maximum reward (0) happens when the pendulum is upright and still.\n\n* Worst case is around -16.","metadata":{"id":"b_lHcIcqUaqB","papermill":{"duration":0.025728,"end_time":"2025-06-19T00:35:13.551812","exception":false,"start_time":"2025-06-19T00:35:13.526084","status":"completed"},"tags":[]}},{"cell_type":"code","source":"print('Action Spec:')\nprint(train_env.action_spec())","metadata":{"id":"bttJ4uxZUQBr","papermill":{"duration":0.033518,"end_time":"2025-06-19T00:35:13.610985","exception":false,"start_time":"2025-06-19T00:35:13.577467","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:10:26.488155Z","iopub.execute_input":"2025-06-28T03:10:26.488405Z","iopub.status.idle":"2025-06-28T03:10:26.502168Z","shell.execute_reply.started":"2025-06-28T03:10:26.488389Z","shell.execute_reply":"2025-06-28T03:10:26.501528Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"This is your action space — what the agent can do.\n\n* It's a single continuous action in [-2.0, 2.0].\n* This represents the torque applied to the pendulum motor.","metadata":{"id":"eJCgJnx3g0yY","papermill":{"duration":0.025475,"end_time":"2025-06-19T00:35:13.662509","exception":false,"start_time":"2025-06-19T00:35:13.637034","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Reiniciar entorno\ntime_step = render_env.reset()\nprint('Time step:')\nprint(time_step)\n\n# Tomar acción continua (en el rango permitido, e.g., [-2.0] a [2.0])\naction = np.array([0.5], dtype=np.float32)\n\n# Ejecutar un paso en el entorno\nnext_time_step = render_env.step(action)\nprint('Next time step:')\nprint(next_time_step)\n","metadata":{"id":"V2UGR5t_iZX-","papermill":{"duration":0.039678,"end_time":"2025-06-19T00:35:13.727742","exception":false,"start_time":"2025-06-19T00:35:13.688064","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:10:26.502936Z","iopub.execute_input":"2025-06-28T03:10:26.503159Z","iopub.status.idle":"2025-06-28T03:10:26.523096Z","shell.execute_reply.started":"2025-06-28T03:10:26.503144Z","shell.execute_reply":"2025-06-28T03:10:26.522361Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Time step:\nThis is the observation returned when the environment is reset. It contains:\n\n* An initial observation: cos(θ), sin(θ), θ_dot\n* An empty dictionary {} — often used for metadata in newer Gymnasium versions.\n\nNext time step:\nThis is what you get after taking one action (0.5 torque):\n\n* Observation → new state of the system.\n* Reward → -0.83 (negative: it’s not upright yet).\n* Terminated → False (no terminal condition was reached).\n* Truncated → False (the episode hasn’t hit max steps).\n* Info dict → {} — again, often unused unless specific settings require it.","metadata":{"id":"4JSc9GviWUBK","papermill":{"duration":0.025772,"end_time":"2025-06-19T00:35:13.779707","exception":false,"start_time":"2025-06-19T00:35:13.753935","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train_py_env = suite_gym.load(env_name)\neval_py_env = suite_gym.load(env_name)","metadata":{"id":"N7brXNIGWXjC","papermill":{"duration":0.035497,"end_time":"2025-06-19T00:35:13.841014","exception":false,"start_time":"2025-06-19T00:35:13.805517","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:10:26.523942Z","iopub.execute_input":"2025-06-28T03:10:26.524216Z","iopub.status.idle":"2025-06-28T03:10:26.544615Z","shell.execute_reply.started":"2025-06-28T03:10:26.524198Z","shell.execute_reply":"2025-06-28T03:10:26.543951Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_env = tf_py_environment.TFPyEnvironment(train_py_env)\neval_env = tf_py_environment.TFPyEnvironment(eval_py_env)","metadata":{"id":"Xp-Y4mD6eDhF","papermill":{"duration":0.052986,"end_time":"2025-06-19T00:35:13.973242","exception":false,"start_time":"2025-06-19T00:35:13.920256","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:10:26.545485Z","iopub.execute_input":"2025-06-28T03:10:26.545760Z","iopub.status.idle":"2025-06-28T03:10:26.568628Z","shell.execute_reply.started":"2025-06-28T03:10:26.545742Z","shell.execute_reply":"2025-06-28T03:10:26.567904Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Summary**\n| What        | Meaning                                       |\n| ----------- | --------------------------------------------- |\n| Obs. Spec   | Your agent sees a 3D vector (cosθ, sinθ, θ̇)  |\n| Action Spec | Agent outputs a float torque ∈ \\[-2, 2]       |\n| Reward      | Negative unless pendulum is upright and still |\n| Step Output | Observation, reward, done flags, info dict    |\n","metadata":{}},{"cell_type":"markdown","source":"## Agent\n\nThe algorithm used to solve an RL problem is represented by an `Agent`. TF-Agents provides standard implementations of a variety of `Agents`, including:\n\n-   [DQN](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)\n-   [REINFORCE](https://www-anw.cs.umass.edu/~barto/courses/cs687/williams92simple.pdf)\n-   [DDPG](https://arxiv.org/pdf/1509.02971.pdf)\n-   [TD3](https://arxiv.org/pdf/1802.09477.pdf)\n-   [PPO](https://arxiv.org/abs/1707.06347)\n-   [SAC](https://arxiv.org/abs/1801.01290)\n\n`Pendulum-v1` has a continuous action space:\n`BoundedTensorSpec(shape=(1,), dtype=tf.float32, minimum=-2.0, maximum=2.0)`\nThis rules out standard DQN (Deep Q-Network), which is only suitable for discrete action spaces. DQN outputs one Q-value per possible action, which doesn't work when the action is a float like 0.35 or -1.2.\n\n**What is DDPG?**\n\nDeep Deterministic Policy Gradient (DDPG) is an actor-critic algorithm designed for continuous control tasks. It combines:\n\n* A deterministic actor that outputs a specific continuous action.\n* A critic that estimates the Q-value of (state, action) pairs.\n\nDDPG is a model-free, off-policy RL algorithm based on:\n* Deep Q-Learning ideas\n* Deterministic Policy Gradient (DPG) theorem\n","metadata":{"id":"E9lW_OZYFR8A","papermill":{"duration":0.026067,"end_time":"2025-06-19T00:35:14.026467","exception":false,"start_time":"2025-06-19T00:35:14.000400","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#hiperparametros de la red\nactor_fc_layers = (400, 300)\ncritic_fc_layers = (400, 300) \ncritic_obs_fc_layers = (400,)\ncritic_action_fc_layers = (300,)\ncritic_joint_fc_layers = (300,)","metadata":{"id":"TgkdEPg_muzV","papermill":{"duration":0.05122,"end_time":"2025-06-19T00:35:14.103875","exception":false,"start_time":"2025-06-19T00:35:14.052655","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:10:26.569399Z","iopub.execute_input":"2025-06-28T03:10:26.569626Z","iopub.status.idle":"2025-06-28T03:10:26.576290Z","shell.execute_reply.started":"2025-06-28T03:10:26.569611Z","shell.execute_reply":"2025-06-28T03:10:26.575695Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#obtener las especificaciones\nobservation_spec = train_env.observation_spec()\naction_spec = train_env.action_spec()\ntime_step_spec = train_env.time_step_spec()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:10:26.576968Z","iopub.execute_input":"2025-06-28T03:10:26.577210Z","iopub.status.idle":"2025-06-28T03:10:26.597317Z","shell.execute_reply.started":"2025-06-28T03:10:26.577194Z","shell.execute_reply":"2025-06-28T03:10:26.596526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class CriticWrapper(network.Network):\n    def __init__(self, observation_spec, action_spec, model, name='CriticWrapper'):\n        input_tensor_spec = (observation_spec, action_spec)\n        super().__init__(input_tensor_spec=input_tensor_spec, state_spec=(), name=name)\n        self._model = model\n\n    def call(self, inputs, step_type=None, network_state=(), training=False):\n        observations, actions = inputs\n        obs_shape = tf.shape(observations)\n        obs_rank = tf.rank(observations)\n\n        def with_time_dim():\n            batch_size = obs_shape[0]\n            seq_len = obs_shape[1]\n            obs_flat = tf.reshape(observations, (batch_size * seq_len, -1))\n            act_flat = tf.reshape(actions, (batch_size * seq_len, -1))\n            q_flat = self._model([obs_flat, act_flat], training=training)\n            return tf.reshape(q_flat, (batch_size, seq_len, 1)), network_state\n\n        def no_time_dim():\n            batch_size = tf.shape(observations)[0]\n            q = self._model([observations, actions], training=training)\n            return tf.reshape(q, (batch_size, 1)), network_state\n\n        return tf.cond(\n            tf.equal(obs_rank, 3),\n            true_fn=with_time_dim,\n            false_fn=no_time_dim\n        )\n\n\n\nclass ActorWrapper(network.Network):\n    def __init__(self, observation_spec, action_spec, model, name='ActorWrapper'):\n        super().__init__(input_tensor_spec=observation_spec, state_spec=(), name=name)\n        self._model = model\n\n    def call(self, observation, step_type=None, network_state=(), training=False):\n        obs_shape = tf.shape(observation)\n        obs_rank = tf.rank(observation)\n\n        def apply_time_dist():\n            batch_size = obs_shape[0]\n            seq_len = obs_shape[1]\n            obs_dim = obs_shape[-1]\n            obs_flat = tf.reshape(observation, (batch_size * seq_len, obs_dim))\n            actions_flat = self._model(obs_flat, training=training)\n            return tf.reshape(actions_flat, (batch_size, seq_len, -1))\n\n        def apply_no_time_dist():\n            return self._model(observation, training=training)\n\n        actions = tf.cond(\n            tf.equal(obs_rank, 3),\n            true_fn=apply_time_dist,\n            false_fn=apply_no_time_dist\n        )\n\n        return actions, network_state\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:11:45.298553Z","iopub.execute_input":"2025-06-28T03:11:45.299095Z","iopub.status.idle":"2025-06-28T03:11:45.309678Z","shell.execute_reply.started":"2025-06-28T03:11:45.299070Z","shell.execute_reply":"2025-06-28T03:11:45.308968Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_actor_model(observation_spec, action_spec, fc_layer_params=(400, 300)):\n    inputs = tf.keras.Input(shape=observation_spec.shape)\n    x = inputs\n    for units in fc_layer_params:\n        x = tf.keras.layers.Dense(units, activation='relu')(x)\n    outputs = tf.keras.layers.Dense(action_spec.shape[0], activation='tanh')(x)  # asumir rango [-1,1]\n    return tf.keras.Model(inputs=inputs, outputs=outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:11:45.312363Z","iopub.execute_input":"2025-06-28T03:11:45.312572Z","iopub.status.idle":"2025-06-28T03:11:45.331477Z","shell.execute_reply.started":"2025-06-28T03:11:45.312556Z","shell.execute_reply":"2025-06-28T03:11:45.330597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def create_critic_network(observation_spec, action_spec, fc_layer_params=(400, 300)):\n    obs_input = tf.keras.Input(shape=observation_spec.shape)\n    action_input = tf.keras.Input(shape=action_spec.shape)\n    concat = tf.keras.layers.Concatenate()([obs_input, action_input])\n    x = concat\n    for units in fc_layer_params:\n        x = tf.keras.layers.Dense(units, activation='relu')(x)\n    q_value = tf.keras.layers.Dense(1)(x)\n    return tf.keras.Model(inputs=[obs_input, action_input], outputs=q_value)\n\n\n# --- Construcción del agente ---\nactor_model = create_actor_model(observation_spec, action_spec, actor_fc_layers)\ntarget_actor_model = create_actor_model(observation_spec, action_spec, actor_fc_layers)\ncritic_model = create_critic_network(observation_spec, action_spec, critic_fc_layers)\ntarget_critic_model = create_critic_network(observation_spec, action_spec, critic_fc_layers)\n\nactor_net = ActorWrapper(observation_spec, action_spec, actor_model)\ntarget_actor_net = ActorWrapper(observation_spec, action_spec, target_actor_model)\ncritic_net = CriticWrapper(observation_spec, action_spec, critic_model)\ntarget_critic_net = CriticWrapper(observation_spec, action_spec, target_critic_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:11:45.332923Z","iopub.execute_input":"2025-06-28T03:11:45.333194Z","iopub.status.idle":"2025-06-28T03:11:45.515440Z","shell.execute_reply.started":"2025-06-28T03:11:45.333172Z","shell.execute_reply":"2025-06-28T03:11:45.514795Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"actor_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\ncritic_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n\ntrain_step_counter = tf.Variable(0)\n\nagent = ddpg_agent.DdpgAgent(\n    time_step_spec,\n    action_spec,\n    actor_network=actor_net,\n    critic_network=critic_net,\n    actor_optimizer=actor_optimizer,\n    critic_optimizer=critic_optimizer,\n    ou_stddev=0.2,\n    ou_damping=0.15,\n    target_actor_network=target_actor_net,\n    target_critic_network=target_critic_net,\n    target_update_tau=0.005,\n    target_update_period=1,\n    gamma=0.99,\n    train_step_counter=train_step_counter\n)\n\nagent.initialize()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:11:45.516708Z","iopub.execute_input":"2025-06-28T03:11:45.517068Z","iopub.status.idle":"2025-06-28T03:11:45.561773Z","shell.execute_reply.started":"2025-06-28T03:11:45.517041Z","shell.execute_reply":"2025-06-28T03:11:45.561173Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"El agente DDPG crea internamente redes objetivo (target networks) como copias de las redes principales (actor y crítico).\n\nCuando usas modelos funcionales de Keras dentro de wrappers personalizados (ActorWrapper, CriticWrapper), es posible que el agente intente copiar el wrapper pero termine compartiendo variables con el modelo original, lo cual no está permitido.\n\nPara evitar esto:\n\n1. Se crean funciones constructoras (build_actor_model, build_critic_model) que generan nuevas instancias del modelo Keras.\n\n2. Se generan dos modelos independientes para cada red (actor y crítico): uno para la red principal y otro para la red objetivo.\n\n3. Cada modelo se envuelve en su propio wrapper (ActorWrapper o CriticWrapper).\n\n4. Se pasa explícitamente la red objetivo al agente DDPG usando los parámetros:\n\ntarget_actor_network\n\ntarget_critic_network\n\nAsí, el agente trabaja con redes independientes que no comparten pesos entre sí, evitando el error y permitiendo entrenar correctamente.","metadata":{}},{"cell_type":"markdown","source":"## Policies\n\nA policy defines the way an agent acts in an environment. Typically, the goal of reinforcement learning is to train the underlying model until the policy produces the desired outcome.\n\n\nAgents contain two policies: \n\n-   `agent.policy` — The main policy that is used for evaluation and deployment.\n-   `agent.collect_policy` — A second policy that is used for data collection.\n","metadata":{"id":"I0KLrEPwkn5x","papermill":{"duration":0.025698,"end_time":"2025-06-19T00:35:14.675569","exception":false,"start_time":"2025-06-19T00:35:14.649871","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Política entrenada (DDPG)\neval_policy = agent.policy\n\n# Política usada para recolectar experiencias\ncollect_policy = agent.collect_policy","metadata":{"id":"BwY7StuMkuV4","papermill":{"duration":0.032373,"end_time":"2025-06-19T00:35:14.733953","exception":false,"start_time":"2025-06-19T00:35:14.701580","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:11:45.562539Z","iopub.execute_input":"2025-06-28T03:11:45.562793Z","iopub.status.idle":"2025-06-28T03:11:45.567020Z","shell.execute_reply.started":"2025-06-28T03:11:45.562765Z","shell.execute_reply":"2025-06-28T03:11:45.566136Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Política aleatoria para comparar desempeño\nrandom_policy = random_tf_policy.RandomTFPolicy(\n    time_step_spec=train_env.time_step_spec(),\n    action_spec=train_env.action_spec()\n)","metadata":{"id":"HE37-UCIrE69","papermill":{"duration":0.032452,"end_time":"2025-06-19T00:35:14.843267","exception":false,"start_time":"2025-06-19T00:35:14.810815","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:11:45.568719Z","iopub.execute_input":"2025-06-28T03:11:45.568976Z","iopub.status.idle":"2025-06-28T03:11:45.582820Z","shell.execute_reply.started":"2025-06-28T03:11:45.568960Z","shell.execute_reply":"2025-06-28T03:11:45.581733Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"time_step = train_env.reset()","metadata":{"id":"D4DHZtq3Ndis","papermill":{"duration":0.034618,"end_time":"2025-06-19T00:35:15.017329","exception":false,"start_time":"2025-06-19T00:35:14.982711","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:11:45.584024Z","iopub.execute_input":"2025-06-28T03:11:45.584346Z","iopub.status.idle":"2025-06-28T03:11:45.596967Z","shell.execute_reply.started":"2025-06-28T03:11:45.584316Z","shell.execute_reply":"2025-06-28T03:11:45.595873Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"random_action = random_policy.action(time_step)","metadata":{"id":"PRFqAUzpNaAW","papermill":{"duration":0.039932,"end_time":"2025-06-19T00:35:15.083052","exception":false,"start_time":"2025-06-19T00:35:15.043120","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:11:45.598037Z","iopub.execute_input":"2025-06-28T03:11:45.598394Z","iopub.status.idle":"2025-06-28T03:11:45.620287Z","shell.execute_reply.started":"2025-06-28T03:11:45.598368Z","shell.execute_reply":"2025-06-28T03:11:45.619196Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Metrics and Evaluation\n\nThe most common metric used to evaluate a policy is the average return. The return is the sum of rewards obtained while running a policy in an environment for an episode. Several episodes are run, creating an average return.\n\nThe following function computes the average return of a policy, given the policy, environment, and a number of episodes.\n","metadata":{"id":"94rCXQtbUbXv","papermill":{"duration":0.025997,"end_time":"2025-06-19T00:35:15.135033","exception":false,"start_time":"2025-06-19T00:35:15.109036","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#@test {\"skip\": true}\ndef compute_avg_return(environment, policy, num_episodes=10):\n\n  total_return = 0.0\n  for _ in range(num_episodes):\n\n    time_step = environment.reset()\n    episode_return = 0.0\n\n    while not time_step.is_last():\n      action_step = policy.action(time_step)\n      time_step = environment.step(action_step.action)\n      episode_return += time_step.reward\n    total_return += episode_return\n\n  avg_return = total_return / num_episodes\n  return avg_return.numpy()\n\n\n# See also the metrics module for standard implementations of different metrics.\n# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics","metadata":{"id":"bitzHo5_UbXy","papermill":{"duration":0.033311,"end_time":"2025-06-19T00:35:15.194222","exception":false,"start_time":"2025-06-19T00:35:15.160911","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:11:45.621012Z","iopub.execute_input":"2025-06-28T03:11:45.621332Z","iopub.status.idle":"2025-06-28T03:11:45.629019Z","shell.execute_reply.started":"2025-06-28T03:11:45.621286Z","shell.execute_reply":"2025-06-28T03:11:45.628226Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Running this computation on the `random_policy` shows a baseline performance in the environment.","metadata":{"id":"_snCVvq5Z8lJ","papermill":{"duration":0.025505,"end_time":"2025-06-19T00:35:15.245384","exception":false,"start_time":"2025-06-19T00:35:15.219879","status":"completed"},"tags":[]}},{"cell_type":"code","source":"compute_avg_return(eval_env, random_policy, num_eval_episodes)","metadata":{"id":"9bgU6Q6BZ8Bp","papermill":{"duration":0.461184,"end_time":"2025-06-19T00:35:15.732476","exception":false,"start_time":"2025-06-19T00:35:15.271292","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:11:45.629817Z","iopub.execute_input":"2025-06-28T03:11:45.630077Z","iopub.status.idle":"2025-06-28T03:11:51.608059Z","shell.execute_reply.started":"2025-06-28T03:11:45.630044Z","shell.execute_reply":"2025-06-28T03:11:51.607206Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Means that your policy (in this case, the random policy) obtained an average return of -1124.45 in a single episode","metadata":{}},{"cell_type":"markdown","source":"## Replay Buffer\n\nIn order to keep track of the data collected from the environment, we will use [Reverb](https://deepmind.com/research/open-source/Reverb), an efficient, extensible, and easy-to-use replay system by Deepmind. It stores experience data when we collect trajectories and is consumed during training.\n\nThis replay buffer is constructed using specs describing the tensors that are to be stored, which can be obtained from the agent using agent.collect_data_spec.\n","metadata":{"id":"NLva6g2jdWgr","papermill":{"duration":0.025881,"end_time":"2025-06-19T00:35:15.785194","exception":false,"start_time":"2025-06-19T00:35:15.759313","status":"completed"},"tags":[]}},{"cell_type":"code","source":"table_name = 'uniform_table'\n\n# Obtiene la firma para las muestras que guardará el replay buffer\nreplay_buffer_signature = tensor_spec.from_spec(agent.collect_data_spec)\nreplay_buffer_signature = tensor_spec.add_outer_dim(replay_buffer_signature)\n\n# Crea la tabla en Reverb para almacenar las muestras\ntable = reverb.Table(\n    table_name,\n    max_size=replay_buffer_max_length,\n    sampler=reverb.selectors.Uniform(),\n    remover=reverb.selectors.Fifo(),\n    rate_limiter=reverb.rate_limiters.MinSize(1),\n    signature=replay_buffer_signature)\n\n# Inicia el servidor Reverb\nreverb_server = reverb.Server([table])\n\n# Crea el replay buffer\nreplay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n    agent.collect_data_spec,\n    table_name=table_name,\n    sequence_length=2,\n    local_server=reverb_server)\n\n# Observador que agrega datos al replay buffer\nrb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n    replay_buffer.py_client,\n    table_name,\n    sequence_length=2)","metadata":{"id":"vX2zGUWJGWAl","papermill":{"duration":0.046217,"end_time":"2025-06-19T00:35:15.857117","exception":false,"start_time":"2025-06-19T00:35:15.810900","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:11:51.609241Z","iopub.execute_input":"2025-06-28T03:11:51.609583Z","iopub.status.idle":"2025-06-28T03:11:51.626209Z","shell.execute_reply.started":"2025-06-28T03:11:51.609555Z","shell.execute_reply":"2025-06-28T03:11:51.625424Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For most agents, `collect_data_spec` is a named tuple called `Trajectory`, containing the specs for observations, actions, rewards, and other items.","metadata":{"id":"ZGNTDJpZs4NN","papermill":{"duration":0.026059,"end_time":"2025-06-19T00:35:15.909506","exception":false,"start_time":"2025-06-19T00:35:15.883447","status":"completed"},"tags":[]}},{"cell_type":"code","source":"agent.collect_data_spec","metadata":{"id":"_IZ-3HcqgE1z","papermill":{"duration":0.035518,"end_time":"2025-06-19T00:35:15.971210","exception":false,"start_time":"2025-06-19T00:35:15.935692","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:11:51.628639Z","iopub.execute_input":"2025-06-28T03:11:51.628886Z","iopub.status.idle":"2025-06-28T03:11:51.641954Z","shell.execute_reply.started":"2025-06-28T03:11:51.628869Z","shell.execute_reply":"2025-06-28T03:11:51.641263Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"agent.collect_data_spec._fields","metadata":{"id":"sy6g1tGcfRlw","papermill":{"duration":0.034599,"end_time":"2025-06-19T00:35:16.032383","exception":false,"start_time":"2025-06-19T00:35:15.997784","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:11:51.642763Z","iopub.execute_input":"2025-06-28T03:11:51.642959Z","iopub.status.idle":"2025-06-28T03:11:51.661030Z","shell.execute_reply.started":"2025-06-28T03:11:51.642945Z","shell.execute_reply":"2025-06-28T03:11:51.660332Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"| Field                | Meaning                                                                                                   |\n| -------------------- | --------------------------------------------------------------------------------------------------------- |\n| **`step_type`**      | An integer enum (`0`, `1`, `2`) indicating whether the step is: <br> `0 = FIRST`, `1 = MID`, `2 = LAST`   |\n| **`observation`**    | The current observation (state) from the environment — for Pendulum: a vector of 3 floats                 |\n| **`action`**         | The action taken by the agent — for Pendulum: a float in `[-2, 2]`                                        |\n| **`policy_info`**    | Additional info the policy may output (like log-probs). Empty `()` in your case, which is normal for DDPG |\n| **`next_step_type`** | The `step_type` of the *next* step (used to track episode progress)                                       |\n| **`reward`**         | The scalar reward received after taking the action                                                        |\n| **`discount`**       | The discount factor (usually `1.0` except at terminal states where it's `0.0`)                            |\n","metadata":{}},{"cell_type":"markdown","source":"## Data Collection\n\nThis part of the code is responsible for collecting initial experiences from the environment using a random policy, which are then stored in the replay buffer. These experiences are necessary to pre-fill the replay buffer before the agent starts learning.","metadata":{"id":"rVD5nQ9ZGo8_","papermill":{"duration":0.02583,"end_time":"2025-06-19T00:35:16.084584","exception":false,"start_time":"2025-06-19T00:35:16.058754","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Usar la política aleatoria para coleccionar pasos iniciales\ninitial_driver = py_driver.PyDriver(\n    train_py_env,  # entorno python (sin wrapper TF)\n    py_tf_eager_policy.PyTFEagerPolicy(\n        random_policy, use_tf_function=True),\n    observers=[rb_observer],  # datos al replay buffer\n    max_steps=initial_collect_steps)\n\n# Ejecutar la colección inicial\ninitial_driver.run(train_py_env.reset())","metadata":{"id":"wr1KSAEGG4h9","papermill":{"duration":0.246111,"end_time":"2025-06-19T00:35:16.356983","exception":false,"start_time":"2025-06-19T00:35:16.110872","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:11:51.661788Z","iopub.execute_input":"2025-06-28T03:11:51.662055Z","iopub.status.idle":"2025-06-28T03:11:53.503768Z","shell.execute_reply.started":"2025-06-28T03:11:51.662037Z","shell.execute_reply":"2025-06-28T03:11:53.503050Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dataset generates trajectories with shape [Bx2x...]\ndataset = replay_buffer.as_dataset(\n    sample_batch_size=batch_size,\n    num_steps=2,\n    num_parallel_calls=3).prefetch(3)","metadata":{"id":"ba7bilizt_qW","papermill":{"duration":0.367181,"end_time":"2025-06-19T00:35:16.916626","exception":false,"start_time":"2025-06-19T00:35:16.549445","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:11:53.504611Z","iopub.execute_input":"2025-06-28T03:11:53.505196Z","iopub.status.idle":"2025-06-28T03:11:53.792685Z","shell.execute_reply.started":"2025-06-28T03:11:53.505174Z","shell.execute_reply":"2025-06-28T03:11:53.791766Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"iterator = iter(dataset)\nprint(iterator)","metadata":{"id":"K13AST-2ppOq","papermill":{"duration":0.110286,"end_time":"2025-06-19T00:35:17.053967","exception":false,"start_time":"2025-06-19T00:35:16.943681","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:11:53.793591Z","iopub.execute_input":"2025-06-28T03:11:53.793877Z","iopub.status.idle":"2025-06-28T03:11:53.899680Z","shell.execute_reply.started":"2025-06-28T03:11:53.793848Z","shell.execute_reply":"2025-06-28T03:11:53.898738Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training the agent\n\nTwo things must happen during the training loop:\n\n-   collect data from the environment\n-   use that data to train the agent's neural network(s)","metadata":{"id":"hBc9lj9VWWtZ","papermill":{"duration":0.026047,"end_time":"2025-06-19T00:35:17.166996","exception":false,"start_time":"2025-06-19T00:35:17.140949","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Optimizar la función de entrenamiento con tf.function\nagent.train = common.function(agent.train)\n\n# Reset contador de pasos\nagent.train_step_counter.assign(0)\n\n# Evaluar política antes de entrenar\navg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\nreturns = [avg_return]\n\n# Resetear entorno python para colección\ntime_step = train_py_env.reset()\n\n# Crear driver para colección de experiencias con política del agente\ncollect_driver = py_driver.PyDriver(\n    train_py_env,  # entorno python puro\n    py_tf_eager_policy.PyTFEagerPolicy(agent.collect_policy, use_tf_function=True),\n    observers=[rb_observer],\n    max_steps=collect_steps_per_iteration)\n\nfor _ in range(num_iterations):\n    # Recolectar experiencias\n    time_step, _ = collect_driver.run(time_step)\n\n    # Obtener un batch de datos del replay buffer\n    experience, unused_info = next(iterator)\n\n    # Entrenar agente con ese batch\n    train_loss = agent.train(experience).loss\n\n    step = agent.train_step_counter.numpy()\n\n    if step % log_interval == 0:\n        print(f'step = {step}: loss = {train_loss}')\n\n    if step % eval_interval == 0:\n        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n        print(f'step = {step}: Average Return = {avg_return}')\n        returns.append(avg_return)","metadata":{"id":"0pTbJ3PeyF-u","papermill":{"duration":359.576988,"end_time":"2025-06-19T00:41:16.770173","exception":false,"start_time":"2025-06-19T00:35:17.193185","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:11:53.900879Z","iopub.execute_input":"2025-06-28T03:11:53.901763Z","iopub.status.idle":"2025-06-28T03:25:09.393534Z","shell.execute_reply.started":"2025-06-28T03:11:53.901739Z","shell.execute_reply":"2025-06-28T03:25:09.392771Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Visualization\n","metadata":{"id":"68jNcA_TiJDq","papermill":{"duration":0.030665,"end_time":"2025-06-19T00:41:16.832085","exception":false,"start_time":"2025-06-19T00:41:16.801420","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"### Plots\n\nUse `matplotlib.pyplot` to chart how the policy improved during training.\n\nOne iteration of `Cartpole-v0` consists of 200 time steps. The environment gives a reward of `+1` for each step the pole stays up, so the maximum return for one episode is 200. The charts shows the return increasing towards that maximum each time it is evaluated during training. (It may be a little unstable and not increase monotonically each time.)","metadata":{"id":"aO-LWCdbbOIC","papermill":{"duration":0.030805,"end_time":"2025-06-19T00:41:16.894226","exception":false,"start_time":"2025-06-19T00:41:16.863421","status":"completed"},"tags":[]}},{"cell_type":"code","source":"#@test {\"skip\": true}\n\niterations = range(0, num_iterations + 1, eval_interval)\nplt.plot(iterations, returns)\nplt.ylabel('Average Return')\nplt.xlabel('Iterations')\nplt.title('Training Performance')\n#plt.ylim(top=250)","metadata":{"id":"NxtL1mbOYCVO","papermill":{"duration":0.284232,"end_time":"2025-06-19T00:41:17.209625","exception":false,"start_time":"2025-06-19T00:41:16.925393","status":"completed"},"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T03:25:09.394418Z","iopub.execute_input":"2025-06-28T03:25:09.394678Z","iopub.status.idle":"2025-06-28T03:25:09.684960Z","shell.execute_reply.started":"2025-06-28T03:25:09.394659Z","shell.execute_reply":"2025-06-28T03:25:09.684191Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Videos","metadata":{"id":"M7-XpPP99Cy7","papermill":{"duration":0.031037,"end_time":"2025-06-19T00:41:17.272656","exception":false,"start_time":"2025-06-19T00:41:17.241619","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def record_policy_video(env, policy, filename, num_episodes=5, fps=30):\n    filename = filename if filename.endswith('.mp4') else filename + '.mp4'\n    with imageio.get_writer(filename, fps=fps) as video:\n        for episode in range(num_episodes):\n            obs, _ = env.reset()\n            done = False\n            \n            # Primer frame\n            frame = env.render()\n            video.append_data(frame)\n            \n            while not done:\n                obs_tensor = tf.convert_to_tensor([obs], dtype=tf.float32)\n                policy_time_step = ts.restart(obs_tensor)\n                \n                action_step = policy.action(policy_time_step)\n                action = action_step.action.numpy()[0]\n                \n                obs, reward, terminated, truncated, info = env.step(action)\n                done = terminated or truncated\n                \n                frame = env.render()\n                video.append_data(frame)\n    print(f\"Video saved to {filename}\")\n\nrecord_policy_video(render_env, agent.policy, \"trained_policy.mp4\")\nrecord_policy_video(render_env, agent.collect_policy, \"random_policy.mp4\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T04:33:57.802910Z","iopub.execute_input":"2025-06-28T04:33:57.803578Z","iopub.status.idle":"2025-06-28T04:34:25.476431Z","shell.execute_reply.started":"2025-06-28T04:33:57.803556Z","shell.execute_reply":"2025-06-28T04:34:25.475452Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(os.path.abspath(\"trained_policy.mp4\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T04:36:31.143075Z","iopub.execute_input":"2025-06-28T04:36:31.143655Z","iopub.status.idle":"2025-06-28T04:36:31.147988Z","shell.execute_reply.started":"2025-06-28T04:36:31.143619Z","shell.execute_reply":"2025-06-28T04:36:31.147113Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(os.path.abspath(\"random_policy.mp4\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-28T04:36:57.861935Z","iopub.execute_input":"2025-06-28T04:36:57.862238Z","iopub.status.idle":"2025-06-28T04:36:57.866528Z","shell.execute_reply.started":"2025-06-28T04:36:57.862218Z","shell.execute_reply":"2025-06-28T04:36:57.865839Z"}},"outputs":[],"execution_count":null}]}